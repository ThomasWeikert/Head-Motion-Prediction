{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "571dfe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd  \n",
    "import random\n",
    "import math as mp\n",
    "import pickle\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from pickle import load\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, Input, Lambda\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU\n",
    "from tensorflow.keras.layers import Dense,Flatten,Dropout, MaxPooling1D\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyquaternion import Quaternion\n",
    "\n",
    "import toml\n",
    "\n",
    "from math import floor\n",
    "\n",
    "from ipynb.fs.full.V06_Utils  import *\n",
    "from ipynb.fs.full.V06_Prepare_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd13f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45c215e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Init Configuration\n",
    "######################################################################\n",
    "\n",
    "config_path = 'C:/Users/weikert1/Documents/Thesis/Project_V06_26072021/Code/V06_config.toml'\n",
    "cfg = toml.load(config_path)\n",
    "\n",
    "# General \n",
    "n_in_seq = cfg['n_in_seq_quat']\n",
    "n_out_seq = cfg['n_out_seq_quat']\n",
    "n_steps = cfg['n_steps'] \n",
    "LAT_in_rows = cfg['LAT_in_rows'] #6*16,66ms= 64ms\n",
    "\n",
    "# model ID --> specify this for every new model\n",
    "model_id = cfg['6_2_model_ID']\n",
    "model_type = cfg['6_model_type']\n",
    "\n",
    "# Directories \n",
    "Path_dataset01_test = cfg['Path_dataset01']\n",
    "Path_results_dataset01_GRU_quat_model_id = cfg['Path_results_dataset01'] + model_type + \"/\" + model_id  + \"/\" \n",
    "\n",
    "# Tensorflow configuration\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9f0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate(q):\n",
    "    mult = tf.constant(np.array([1,-1,-1,-1])[np.newaxis], dtype=np.float32)\n",
    "    return q*mult\n",
    "\n",
    "def inverse(q):\n",
    "    return conjugate(q) / tf.reduce_sum(q*q, axis=-1, keepdims=True)\n",
    "\n",
    "def log(q):\n",
    "    v = q[:, 1:]\n",
    "    a = q[:, :1]\n",
    "    q_norm = tf.norm(q, axis=-1, keepdims=True)\n",
    "    x = a / q_norm\n",
    "    eps = np.finfo(np.float32).eps * 8.0\n",
    "    x *= (1.0 - eps)\n",
    "    vec_part = tf.nn.l2_normalize(v, axis=-1) * tf.math.acos(x)\n",
    "    real_part = tf.math.log(q_norm)\n",
    "    return tf.concat([real_part, vec_part], axis=-1)\n",
    "\n",
    "def mult(quaternion1, quaternion2):\n",
    "    w1, x1, y1, z1 = tf.unstack(quaternion1, axis=-1)\n",
    "    w2, x2, y2, z2 = tf.unstack(quaternion2, axis=-1)\n",
    "    x = x1 * w2 + y1 * z2 - z1 * y2 + w1 * x2\n",
    "    y = -x1 * z2 + y1 * w2 + z1 * x2 + w1 * y2\n",
    "    z = x1 * y2 - y1 * x2 + z1 * w2 + w1 * z2\n",
    "    w = -x1 * x2 - y1 * y2 - z1 * z2 + w1 * w2\n",
    "    return tf.stack((w, x, y, z), axis=-1)\n",
    "    \n",
    "def geodesic_dist(q1, q2):\n",
    "    #q1 = q1/tf.norm(q1)\n",
    "    #q2 = q2/tf.norm(q2)\n",
    "    #q2  = K.print_tensor(q2, message='q2  = ')\n",
    "    #q1  = K.print_tensor(q1, message='q1  = ')\n",
    "    #q1_printer = tf.strings.as_string(q1)\n",
    "    #q2_printer = tf.strings.as_string(q2)\n",
    "    #tf.io.write_file(Path_results_dataset01_GRU_quat_model_id + individual_name + \"_q1_Tensors.csv\", str(q1_printer), name=None)\n",
    "    #tf.io.write_file(Path_results_dataset01_GRU_quat_model_id + individual_name + \"_q2_Tensors.csv\", str(q2_printer), name=None)\n",
    "\n",
    "    x = mult(inverse(q1), q2)\n",
    "    x = tf.norm(log(x), axis=-1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def angle_dist(q1, q2):\n",
    "    x = tf.reduce_sum(q1*q2, axis=-1)\n",
    "    eps = np.finfo(np.float32).eps * 8.0\n",
    "    x *= (1.0 - eps)\n",
    "    x = 2*tf.math.acos(x)\n",
    "    return tf.reduce_mean(x)\n",
    "\n",
    "\n",
    "def quat_antipodal_loss(y_true, y_pred):\n",
    "    dist1 = tf.reduce_mean(tf.abs(y_true-y_pred), axis=-1)\n",
    "    dist2 = tf.reduce_mean(tf.abs(y_true+y_pred), axis=-1)\n",
    "    loss = tf.where(dist1<dist2, dist1, dist2)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def euler_angles_loss(y_true, y_pred):\n",
    "    dist1 = tf.abs(y_true - y_pred)\n",
    "    dist2 = tf.abs(2*np.pi + y_true - y_pred)\n",
    "    dist3 = tf.abs(-2*np.pi + y_true - y_pred)\n",
    "    loss = tf.where(dist1<dist2, dist1, dist2)\n",
    "    loss = tf.where(loss<dist3, loss, dist3)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def mean_angle_btw_vectors(v1, v2):\n",
    "    dot_product = tf.reduce_sum(v1*v2, axis=-1)\n",
    "    cos_a = dot_product / (tf.norm(v1, axis=-1) * tf.norm(v2, axis=-1))\n",
    "    eps = 1e-8\n",
    "    cos_a = tf.clip_by_value(cos_a, -1 + eps, 1 - eps)\n",
    "    angle_dist = tf.math.acos(cos_a) / np.pi * 180.0\n",
    "    return tf.reduce_mean(angle_dist)\n",
    "\n",
    "def minmax_inverse_transform(y_true, y_pred):\n",
    "    \n",
    "    w_min, x_min, y_min, z_min, w_max, x_max, y_max, z_max = 0,0,0,0,1,1,1,1\n",
    "    \n",
    "    w1, x1, y1, z1 = tf.unstack(y_true, axis=-1)\n",
    "    w2, x2, y2, z2 = tf.unstack(y_pred, axis=-1)\n",
    "    \n",
    "    w1, w2 = (w1 - K.constant(w_min)) / K.constant(w_max - w_min) , (w2 - K.constant(w_min)) / K.constant(w_max - w_min)\n",
    "    x1, x2 = (x1 - K.constant(x_min)) / K.constant(x_max - x_min) , (x2 - K.constant(x_min)) / K.constant(x_max - x_min)\n",
    "    y1, y2 = (y1 - K.constant(y_min)) / K.constant(y_max - y_min) , (y2 - K.constant(y_min)) / K.constant(y_max - y_min)\n",
    "    z1, z2 = (z1 - K.constant(z_min)) / K.constant(z_max - z_min) , (z2 - K.constant(z_min)) / K.constant(z_max - z_min)\n",
    "\n",
    "    y_true = tf.stack((w1, x1, y1, z1), axis=-1)\n",
    "    y_pred = tf.stack((w2, x2, y2, z2), axis=-1)\n",
    "    return y_true, y_pred\n",
    "\n",
    "def ang_mae(y_true,y_pred):\n",
    "    ang_dist = geodesic_dist(y_true, y_pred) #returns a value between 0 and pi\n",
    "    #ang_dist_deg = rad2deg(ang_dist_rad)\n",
    "    shape = tf.cast(tf.size(ang_dist),dtype=np.float32)\n",
    "    ang_mae =  K.sum(ang_dist)/shape\n",
    "    #y_pred  = K.print_tensor(y_pred, message='y_pred  = ')\n",
    "    #y_true  = K.print_tensor(y_true, message='y_true  = ')\n",
    "    return tf.reduce_mean(ang_mae)\n",
    "\n",
    "def rad2deg(rad):\n",
    "    pi_on_180 = 0.017453292519943295\n",
    "    return rad / pi_on_180\n",
    "\n",
    "\n",
    "def absolute_distance(q0, q1):\n",
    "    q0_minus_q1 = q0 - q1\n",
    "    q0_plus_q1  = q0 + q1\n",
    "    d_minus =  tf.norm(q0_minus_q1, axis=-1, keepdims=True) \n",
    "    d_plus  = tf.norm(q0_plus_q1, axis=-1, keepdims=True) \n",
    "    check = tf.math.less(d_minus, d_plus)\n",
    "    result = tf.where(check == True , d_minus, d_plus)\n",
    "    q0  = K.print_tensor(q0, message='q0  = ')\n",
    "    q1  = K.print_tensor(q1, message='q1  = ')\n",
    "    return tf.reduce_mean(result)\n",
    "\n",
    "\n",
    "def rotation_loss(y_true, y_pred):\n",
    "    #q1 = q1/tf.norm(q1)\n",
    "    #q2 = q2/tf.norm(q2)\n",
    "    #y_pred  = K.print_tensor(y_pred, message='y_pred  = ')\n",
    "    #y_true  = K.print_tensor(y_true, message='y_true  = ')\n",
    "    #y_pred = y_pred/tf.norm(y_pred)\n",
    "    #y_true = y_true/tf.norm(y_true)\n",
    "    loss = y_true - y_pred\n",
    "    loss = tf.norm(loss)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def rotation_loss_2(y_true,y_pred):\n",
    "    rotation_loss = tf.sqrt(tf.nn.l2_loss(y_true-y_pred))\n",
    "    return tf.reduce_mean(rotation_loss)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d93cea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU_v01(X_train, y_train, Neurons, Dropout_rate,Learning_rate, Decay):\n",
    "\n",
    "    # the dataset knows the number of features, e.g. 2\n",
    "    n_steps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "\n",
    "\n",
    "    ...\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNGRU(Neurons, input_shape=(n_steps, n_features),return_state=False , return_sequences=True))\n",
    "    #model.add(Dropout(Dropout_rate))\n",
    "    \n",
    "    model.add(CuDNNGRU(Neurons,return_state=False , return_sequences=False))\n",
    "    #model.add(Dropout(Dropout_rate))\n",
    "        \n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    #model.add(Dense(Neurons, activation='relu'))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    model.add(Dense(n_features))\n",
    "    \n",
    "    #opt = tf.keras.optimizers.Adamax(learning_rate=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\")\n",
    "    #opt = tf.keras.optimizers.SGD(lr=0.01, momentum=0.5, decay=0.001)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=Learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=Decay)\n",
    "    \n",
    "    \n",
    "    model.compile(opt, loss=geodesic_dist) #run_eagerly=True\n",
    "    \n",
    "    #model.train_step = make_print_data_and_train_step(model)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0236b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Baseline(X_train, y_train):\n",
    "\n",
    "    # the dataset knows the number of features, e.g. 2\n",
    "    n_steps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "    \n",
    "    # Slice over each sequence and receive the last input\n",
    "    # call groundtruth \n",
    "    # call evaluation metrics from \n",
    "    \n",
    "    \n",
    "    input_lyr = Input(shape=(n_steps, n_features), dtype=tf.float32)\n",
    "    sliced_lyr = Lambda(lambda X_train: X_train[:,-1,:])\n",
    "    \n",
    "    \n",
    "    baseline = Sequential([\n",
    "    input_lyr,\n",
    "    sliced_lyr,\n",
    "    ])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    #opt = tf.keras.optimizers.Adam(learning_rate=Learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=Decay)\n",
    "    \n",
    "    baseline.compile(loss=geodesic_dist, run_eagerly=True) #run_eagerly=True\n",
    "    \n",
    "    #baseline.train_step = make_print_data_and_train_step(baseline)\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    return baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "743f93f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "# serialize model to YAML\n",
    "    model_yaml = model.to_yaml()\n",
    "    with open(path +  \"_CUDNN_arch.yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "    model.save_weights(path + \"_CUDNN_best_model.h5\")\n",
    "    return print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1519d",
   "metadata": {},
   "source": [
    "# Train the mdel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9d87017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35290939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dedcff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.keras.engine import data_adapter\n",
    "\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(\"For batch {}, loss is {:7.4f}.\".format(batch, logs[\"loss\"]))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print(\"For batch {}, loss is {:7.4f}.\".format(batch, logs[\"loss\"]))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"The average loss for epoch {} is {:7.4f} \".format(epoch, logs[\"loss\"]))\n",
    "    \n",
    "        \n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def make_print_data_and_train_step(keras_model):\n",
    "            original_train_step = keras_model.train_step\n",
    "\n",
    "            def print_data_and_train_step(original_data):\n",
    "                # Basically copied one-to-one from https://git.io/JvDTv\n",
    "                data = data_adapter.expand_1d(original_data)\n",
    "                x, y_true, w = data_adapter.unpack_x_y_sample_weight(data)\n",
    "                y_pred = keras_model(x, training=True)\n",
    "\n",
    "                # this is pretty much like on_train_batch_begin\n",
    "                K.print_tensor(w, \"Sample weight (w) =\")\n",
    "                K.print_tensor(x, \"Batch input (x) =\")\n",
    "                K.print_tensor(y_true, \"Batch output (y_true) =\")\n",
    "                K.print_tensor(y_pred, \"Prediction (y_pred) =\")\n",
    "\n",
    "                result = original_train_step(original_data)     \n",
    "\n",
    "\n",
    "                # add anything here for on_train_batch_end-like behavior\n",
    "\n",
    "                y_true_np = y_true.numpy()\n",
    "                y_true_list.append(y_true_np)\n",
    "                y_true_df = pd.DataFrame(np.row_stack(y_true_list), columns= cfg['quat_coords_true'])\n",
    "                y_true_df.to_csv(os.path.join(Path_results_dataset01_GRU_quat_model_id + 'y_true.csv'), index=False)\n",
    "\n",
    "                y_pred_np = y_pred.numpy()\n",
    "                y_pred_list.append(y_pred_np)\n",
    "                y_pred_df = pd.DataFrame(np.row_stack(y_pred_list), columns= cfg['quat_coords_pred'])\n",
    "                y_pred_df.to_csv(os.path.join(Path_results_dataset01_GRU_quat_model_id + 'y_pred.csv'), index=False)\n",
    "\n",
    "\n",
    "                return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            return print_data_and_train_step\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbd457ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAT_in_rows = [1,2,3,4,5,6,7,8,9,10]\n",
    "Neurons = [32]\n",
    "Dropout_rate = [0.5]\n",
    "Learning_rate = [0.05]\n",
    "Batch_size = [256]\n",
    "Epochs = [100]\n",
    "Decay = [0.01]\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22f84313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- GRU_v01 --------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prepare_training_data_quat_special_delta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-7f900371feec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                         \u001b[1;31m#### GRU_v01 #######################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"------------- GRU_v01 --------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_training_data_quat_special_delta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath_dataset01_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGRU_v01\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNeurons_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout_rate_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLearning_rate_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDecay_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prepare_training_data_quat_special_delta' is not defined"
     ]
    }
   ],
   "source": [
    "for Neurons_i in Neurons:\n",
    "    for Dropout_rate_i in Dropout_rate:\n",
    "        for Learning_rate_i in Learning_rate:\n",
    "            for Batch_size_i in Batch_size:\n",
    "                for Epochs_i in Epochs:\n",
    "                    for Decay_i in Decay:\n",
    "                    \n",
    "                        tf.random.set_seed(0)\n",
    "                    \n",
    "                        \n",
    "                        #### GRU_v01 #######################################\n",
    "                        print(\"------------- GRU_v01 --------------------\")\n",
    "                        X_train, X_val, y_train, y_val = prepare_training_data_quat_special_delta(Path_dataset01_test)\n",
    "                        \n",
    "                        model = GRU_v01(X_train, y_train, Neurons_i, Dropout_rate_i,Learning_rate_i, Decay_i)\n",
    "                        \n",
    "                        \n",
    "                        individual_name = \"GRU_v01_Neurons_\" + str(Neurons_i) + \"_\" + \"_DR_\" + str(Dropout_rate_i) + \"__\" + \"LR_\" + str(Learning_rate_i) + \"__\" + \"Batch_\" + str(Batch_size_i) + \"__\" + \"Epochs_\" + str(Epochs_i) \n",
    "                        \n",
    "                        # callbacks parameter\n",
    "                        #es = EarlyStopping(monitor='val_loss', mode='min', patience=10)\n",
    "                        #mc = ModelCheckpoint(Path_results_dataset01_GRU_quat_model_id + \"/Model/\" + individual_name +  '_CUDNN_best_model.h5', monitor ='val_loss', mode='min', save_best_only=True, verbose=0)\n",
    "                        #csv_logger = CSVLogger(Path_results_dataset01_GRU_quat_model_id + \"/Model/\" + individual_name + '.csv', append=True, separator=';')\n",
    "                        \n",
    "                        # validation_data=(X_val, y_val)\n",
    "                        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), shuffle = False, batch_size=Batch_size_i, epochs=Epochs_i, verbose=1) #callbacks=[es,mc,csv_logger] , callbacks=[CustomCallback()]\n",
    "                        #val_history = model.evaluate(X_val, y_val, batch_size=1, verbose=1) #callbacks=[es,mc,csv_logger]\n",
    "\n",
    "                        save_model(model,Path_results_dataset01_GRU_quat_model_id + \"/Model/\" + individual_name )\n",
    "                        \n",
    "                        \n",
    "                        loss_history = history.history[\"loss\"]\n",
    "                        plt.plot(history.history['loss'])\n",
    "                        plt.plot(history.history['val_loss'])\n",
    "                        plt.title('model loss')\n",
    "                        plt.ylabel('loss')\n",
    "                        plt.xlabel('epoch')\n",
    "                        plt.legend(['train', 'test'], loc='upper left')\n",
    "                        plt.show()\n",
    "                        \n",
    "                        loss_history = np.array(history.history[\"loss\"]) \n",
    "                        df_loss = pd.DataFrame(loss_history)\n",
    "                        df_loss = df_loss[df_loss > 0.2]\n",
    "                        df_loss = df_loss[df_loss < 0.2]\n",
    "                        \n",
    "                        val_loss_history = np.array(history.history[\"val_loss\"]) \n",
    "                        df_val_loss = pd.DataFrame(val_loss_history)\n",
    "                        df_val_loss = df_val_loss[df_loss > 0.2]\n",
    "                        df_val_loss = df_val_loss[df_loss < 0.2]\n",
    "\n",
    "                \n",
    "                        plt.plot(df_loss)\n",
    "                        plt.plot(df_val_loss)\n",
    "                        plt.title('model loss')\n",
    "                        plt.ylabel('loss')\n",
    "                        plt.xlabel('epoch')\n",
    "                        plt.legend(['train', 'test'], loc='upper left')\n",
    "                        plt.show()\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        tf.keras.backend.clear_session()\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02535ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Baseline --------------------\n",
      "C:/Users/weikert1/Documents/Thesis/Project_V06_26072021/Data/01_Dataset\\1_special_train_continious.csv\n",
      "24495\n",
      "24494\n",
      "X shape is: (24465, 30, 4)\n",
      "y shape is: (24465, 4)\n",
      "------\n",
      "X shape is: (24465, 30, 4)\n",
      "y shape is: (24465, 4)\n",
      "------\n",
      "C:/Users/weikert1/Documents/Thesis/Project_V06_26072021/Data/01_Dataset\\1_special_test_continious.csv\n",
      "10495\n",
      "10494\n",
      "X shape is: (10465, 30, 4)\n",
      "y shape is: (10465, 4)\n",
      "------\n",
      "X shape is: (10465, 30, 4)\n",
      "y shape is: (10465, 4)\n",
      "------\n",
      "------------- Baseline --------------------\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Epoch 1/100\n",
      "96/96 [==============================] - 1s 12ms/step - loss: 0.0282 - val_loss: 0.0378\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 1s 11ms/step - loss: 0.0282 - val_loss: 0.0378\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 1s 11ms/step - loss: 0.0282 - val_loss: 0.0378\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 1s 11ms/step - loss: 0.0282 - val_loss: 0.0378\n",
      "Epoch 5/100\n",
      "53/96 [===============>..............] - ETA: 0s - loss: 0.0348"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9c5b005b0f68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                         \u001b[0mbaseline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaseline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                         \u001b[0mbaseline_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBatch_size_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEpochs_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#callbacks=[es,mc,csv_logger] , callbacks=[CustomCallback()]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m         \u001b[1;34m\"\"\"Runs a training execution with one step.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mstep_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m       \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    846\u001b[0m       outputs = reduce_per_replica(\n\u001b[0;32m    847\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1283\u001b[0m       fn = autograph.tf_convert(\n\u001b[0;32m   1284\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m-> 1285\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2831\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2832\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2833\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2835\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3606\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3607\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3608\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3610\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mrun_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m         \u001b[1;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m       \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m       loss = self.compiled_loss(\n\u001b[0m\u001b[0;32m    797\u001b[0m           y, y_pred, sample_weight, regularization_losses=self.losses)\n\u001b[0;32m    798\u001b[0m     \u001b[1;31m# Run backwards pass.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001b[0m\n\u001b[0;32m    240\u001b[0m           loss_metric_values)\n\u001b[0;32m    241\u001b[0m       \u001b[0mtotal_loss_metric_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_metric_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m       self._loss_metric.update_state(\n\u001b[0m\u001b[0;32m    243\u001b[0m           total_loss_metric_value, sample_weight=batch_dim)\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# update_op will be None in eager execution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mupdate_state_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mcontrol_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mag_update_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_update_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrol_status\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mag_update_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdef_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Allowlisted %s: from cache'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[1;34m(self, values, sample_weight)\u001b[0m\n\u001b[0;32m    421\u001b[0m       \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m     \u001b[0mvalue_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue_sum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m       \u001b[0mupdate_total_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[1;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[0;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2118\u001b[0m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[1;32m-> 2119\u001b[1;33m                               _ReductionDims(input_tensor, axis))\n\u001b[0m\u001b[0;32m   2120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_ReductionDims\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m   1957\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# Otherwise, we rely on Range and Rank to do the right thing at run-time.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1960\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mrange\u001b[1;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[0;32m   1927\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1929\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m_range\u001b[1;34m(start, limit, delta, name)\u001b[0m\n\u001b[0;32m   7357\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7358\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7359\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   7360\u001b[0m         _ctx, \"Range\", name, start, limit, delta)\n\u001b[0;32m   7361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for Neurons_i in Neurons:\n",
    "    for Dropout_rate_i in Dropout_rate:\n",
    "        for Learning_rate_i in Learning_rate:\n",
    "            for Batch_size_i in Batch_size:\n",
    "                for Epochs_i in Epochs:\n",
    "                    for Decay_i in Decay:\n",
    "                    \n",
    "                        tf.random.set_seed(0)\n",
    "                    \n",
    "                        \n",
    "                        #### Baseline #######################################\n",
    "                        print(\"------------- Baseline --------------------\")\n",
    "                        X_train, X_val, y_train, y_val = prepare_training_data_quat_special(Path_dataset01_test)\n",
    "                        \n",
    "                        #### Baseline #######################################\n",
    "                        print(\"------------- Baseline --------------------\")\n",
    "                    \n",
    "                        baseline = Baseline(X_train, y_train)\n",
    "                            \n",
    "                        baseline_history = baseline.fit(X_train, y_train, validation_data=(X_val, y_val), shuffle = False, batch_size=Batch_size_i, epochs=Epochs_i, verbose=1) #callbacks=[es,mc,csv_logger] , callbacks=[CustomCallback()]\n",
    "                    \n",
    "                        tf.keras.backend.clear_session()\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "f1138154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- GRU_v01 --------------------\n",
      "C:/Users/weikert1/Documents/Thesis/Project_V06_26072021/Data/01_Dataset\\1_special_train.csv\n",
      "24496\n",
      "24495\n",
      "X shape is: (24466, 30, 4)\n",
      "y shape is: (24466, 4)\n",
      "------\n",
      "X shape is: (24466, 30, 4)\n",
      "y shape is: (24466, 4)\n",
      "------\n",
      "C:/Users/weikert1/Documents/Thesis/Project_V06_26072021/Data/01_Dataset\\1_special_test.csv\n",
      "10496\n",
      "10495\n",
      "X shape is: (10466, 30, 4)\n",
      "y shape is: (10466, 4)\n",
      "------\n",
      "X shape is: (10466, 30, 4)\n",
      "y shape is: (10466, 4)\n",
      "------\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnngru (CuDNNGRU)         (None, 30, 32)            3648      \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       (None, 32)                6336      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 10,116\n",
      "Trainable params: 10,116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "96/96 [==============================] - 3s 23ms/step - loss: 0.6162 - val_loss: 0.6217\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.2215 - val_loss: 0.3050\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1759 - val_loss: 0.2385\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1613 - val_loss: 0.2039\n",
      "Epoch 5/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1522 - val_loss: 0.2092\n",
      "Epoch 6/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1397 - val_loss: 0.1603\n",
      "Epoch 7/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1310 - val_loss: 0.1326\n",
      "Epoch 8/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1250 - val_loss: 0.1311\n",
      "Epoch 9/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1214 - val_loss: 0.1150\n",
      "Epoch 10/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1204 - val_loss: 0.1304\n",
      "Epoch 11/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1165 - val_loss: 0.1178\n",
      "Epoch 12/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1185 - val_loss: 0.1218\n",
      "Epoch 13/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1205 - val_loss: 0.1426\n",
      "Epoch 14/100\n",
      "96/96 [==============================] - 2s 18ms/step - loss: 0.1151 - val_loss: 0.1135\n",
      "Epoch 15/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1145 - val_loss: 0.1406\n",
      "Epoch 16/100\n",
      "96/96 [==============================] - 2s 18ms/step - loss: 0.1146 - val_loss: 0.1067\n",
      "Epoch 17/100\n",
      "96/96 [==============================] - 2s 18ms/step - loss: 0.1100 - val_loss: 0.1130\n",
      "Epoch 18/100\n",
      "96/96 [==============================] - 2s 18ms/step - loss: 0.1102 - val_loss: 0.1099\n",
      "Epoch 19/100\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.1115 - val_loss: 0.1178\n",
      "Epoch 20/100\n",
      "96/96 [==============================] - 2s 18ms/step - loss: 0.1113 - val_loss: 0.1061\n",
      "Epoch 21/100\n",
      "96/96 [==============================] - 1s 14ms/step - loss: 0.1091 - val_loss: 0.1115\n",
      "Epoch 22/100\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.1103 - val_loss: 0.1077\n",
      "Epoch 23/100\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.1095 - val_loss: 0.1118\n",
      "Epoch 24/100\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.1090 - val_loss: 0.1019\n",
      "Epoch 25/100\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.1079 - val_loss: 0.1132\n",
      "Epoch 26/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1085 - val_loss: 0.0986\n",
      "Epoch 27/100\n",
      "96/96 [==============================] - 2s 18ms/step - loss: 0.1071 - val_loss: 0.1085\n",
      "Epoch 28/100\n",
      "96/96 [==============================] - 2s 18ms/step - loss: 0.1075 - val_loss: 0.1006\n",
      "Epoch 29/100\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.1064 - val_loss: 0.1046\n",
      "Epoch 30/100\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.1069 - val_loss: 0.1007\n",
      "Epoch 31/100\n",
      "96/96 [==============================] - 2s 18ms/step - loss: 0.1058 - val_loss: 0.1059\n",
      "Epoch 32/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1063 - val_loss: 0.1014\n",
      "Epoch 33/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1054 - val_loss: 0.1042\n",
      "Epoch 34/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1063 - val_loss: 0.1021\n",
      "Epoch 35/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1048 - val_loss: 0.1007\n",
      "Epoch 36/100\n",
      "96/96 [==============================] - 2s 21ms/step - loss: 0.1055 - val_loss: 0.1031\n",
      "Epoch 37/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1044 - val_loss: 0.0992\n",
      "Epoch 38/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1055 - val_loss: 0.1027\n",
      "Epoch 39/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1037 - val_loss: 0.0988\n",
      "Epoch 40/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1045 - val_loss: 0.1023\n",
      "Epoch 41/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1032 - val_loss: 0.0989\n",
      "Epoch 42/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1046 - val_loss: 0.1006\n",
      "Epoch 43/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1028 - val_loss: 0.0984\n",
      "Epoch 44/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1033 - val_loss: 0.1002\n",
      "Epoch 45/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1025 - val_loss: 0.0989\n",
      "Epoch 46/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1036 - val_loss: 0.0994\n",
      "Epoch 47/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1021 - val_loss: 0.0979\n",
      "Epoch 48/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1023 - val_loss: 0.0986\n",
      "Epoch 49/100\n",
      "96/96 [==============================] - 2s 21ms/step - loss: 0.1019 - val_loss: 0.0992\n",
      "Epoch 50/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1026 - val_loss: 0.0979\n",
      "Epoch 51/100\n",
      "96/96 [==============================] - 2s 21ms/step - loss: 0.1016 - val_loss: 0.0988\n",
      "Epoch 52/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1019 - val_loss: 0.0976\n",
      "Epoch 53/100\n",
      "96/96 [==============================] - 2s 21ms/step - loss: 0.1015 - val_loss: 0.0981\n",
      "Epoch 54/100\n",
      "96/96 [==============================] - 2s 21ms/step - loss: 0.1017 - val_loss: 0.0975\n",
      "Epoch 55/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1013 - val_loss: 0.0982\n",
      "Epoch 56/100\n",
      "96/96 [==============================] - 2s 18ms/step - loss: 0.1014 - val_loss: 0.0973\n",
      "Epoch 57/100\n",
      "96/96 [==============================] - 2s 21ms/step - loss: 0.1011 - val_loss: 0.0977\n",
      "Epoch 58/100\n",
      "96/96 [==============================] - 2s 21ms/step - loss: 0.1011 - val_loss: 0.0969\n",
      "Epoch 59/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1009 - val_loss: 0.0973\n",
      "Epoch 60/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1009 - val_loss: 0.0965\n",
      "Epoch 61/100\n",
      "96/96 [==============================] - 2s 21ms/step - loss: 0.1007 - val_loss: 0.0969\n",
      "Epoch 62/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1006 - val_loss: 0.0960\n",
      "Epoch 63/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1004 - val_loss: 0.0964\n",
      "Epoch 64/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.1004 - val_loss: 0.0955\n",
      "Epoch 65/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1002 - val_loss: 0.0959\n",
      "Epoch 66/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1002 - val_loss: 0.0950\n",
      "Epoch 67/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1000 - val_loss: 0.0955\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 2s 20ms/step - loss: 0.1000 - val_loss: 0.0946\n",
      "Epoch 69/100\n",
      "96/96 [==============================] - 2s 21ms/step - loss: 0.0999 - val_loss: 0.0951\n",
      "Epoch 70/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.0998 - val_loss: 0.0942\n",
      "Epoch 71/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.0997 - val_loss: 0.0947\n",
      "Epoch 72/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.0997 - val_loss: 0.0939\n",
      "Epoch 73/100\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.0996 - val_loss: 0.0944\n",
      "Epoch 74/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.0996 - val_loss: 0.0936\n",
      "Epoch 75/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.0994 - val_loss: 0.0942\n",
      "Epoch 76/100\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.0994 - val_loss: 0.0933\n",
      "Epoch 77/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.0993 - val_loss: 0.0941\n",
      "Epoch 78/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.0993 - val_loss: 0.0931\n",
      "Epoch 79/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.0992 - val_loss: 0.0940\n",
      "Epoch 80/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.0992 - val_loss: 0.0930\n",
      "Epoch 81/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.0990 - val_loss: 0.0939\n",
      "Epoch 82/100\n",
      "96/96 [==============================] - 2s 18ms/step - loss: 0.0991 - val_loss: 0.0931\n",
      "Epoch 83/100\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.0990 - val_loss: 0.0938\n",
      "Epoch 84/100\n",
      "96/96 [==============================] - 2s 18ms/step - loss: 0.0990 - val_loss: 0.0933\n",
      "Epoch 85/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.0989 - val_loss: 0.0938\n",
      "Epoch 86/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.0989 - val_loss: 0.0936\n",
      "Epoch 87/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.0988 - val_loss: 0.0940\n",
      "Epoch 88/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.0988 - val_loss: 0.0941\n",
      "Epoch 89/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.0988 - val_loss: 0.0943\n",
      "Epoch 90/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.0987 - val_loss: 0.0945\n",
      "Epoch 91/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.0987 - val_loss: 0.0946\n",
      "Epoch 92/100\n",
      "96/96 [==============================] - 2s 18ms/step - loss: 0.0987 - val_loss: 0.0946\n",
      "Epoch 93/100\n",
      "96/96 [==============================] - 2s 18ms/step - loss: 0.0986 - val_loss: 0.0944\n",
      "Epoch 94/100\n",
      "96/96 [==============================] - 2s 21ms/step - loss: 0.0986 - val_loss: 0.0942\n",
      "Epoch 95/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.0985 - val_loss: 0.0937\n",
      "Epoch 96/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.0985 - val_loss: 0.0934\n",
      "Epoch 97/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.0984 - val_loss: 0.0921\n",
      "Epoch 98/100\n",
      "96/96 [==============================] - 2s 20ms/step - loss: 0.0984 - val_loss: 0.0924\n",
      "Epoch 99/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.0983 - val_loss: 0.0906\n",
      "Epoch 100/100\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.0982 - val_loss: 0.0909\n",
      "Saved model to disk\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAs/ElEQVR4nO3deZgdZZ3//fe36pzeks6+7wESSFgMEMKqgywSQAkzaASFUWfGyPPII/MTeYRRGWEcxxlHxy0ugMygCMiqGQmCIIvIlhAhhARIiInp7Hu6k17O8v39UdWd002HdEhXTtL1eV1XX+fUcqru6pPUp+/7rrrL3B0REUmvoNwFEBGR8lIQiIiknIJARCTlFAQiIimnIBARSTkFgYhIyikIRLrIzP7HzL7WxXVXmNk5+7sdkQNBQSAiknIKAhGRlFMQSI8SN8lca2YLzWynmf3UzIaa2cNmVm9mj5lZ/5L1LzKz18xsm5k9aWaTSpYdb2YL4s/9EqjqsK8PmtnL8WefNbPj3mWZP21my8xsi5nNMbMR8Xwzs/8ysw1mtsPMXjWzY+JlF5jZ4rhsq83sC+/qFyaCgkB6pkuAc4GJwIeAh4F/AgYT/Zv/HICZTQTuAv4xXjYX+F8zqzCzCuBXwM+BAcC98XaJP3s8cBvwGWAg8BNgjplV7ktBzews4N+AmcBwYCVwd7z4A8D74uPoG6+zOV72U+Az7l4LHAP8fl/2K1JKQSA90ffdfb27rwb+ALzg7n9y9ybgQeD4eL2PAg+5++/cPQf8J1ANnAacAmSB77h7zt3vA+aV7GMW8BN3f8HdC+5+O9Acf25ffBy4zd0XuHszcD1wqpmNA3JALXAUYO6+xN3Xxp/LAZPNrI+7b3X3Bfu4X5E2CgLpidaXvG/sZLp3/H4E0V/gALh7EVgFjIyXrfb2ozKuLHk/FrgmbhbaZmbbgNHx5/ZFxzI0EP3VP9Ldfw/8AJgNbDCzm82sT7zqJcAFwEoze8rMTt3H/Yq0URBImq0hOqEDUZs80cl8NbAWGBnPazWm5P0q4F/dvV/JT42737WfZehF1NS0GsDdv+fuJwKTiZqIro3nz3P3GcAQoiase/ZxvyJtFASSZvcAF5rZ2WaWBa4hat55FngOyAOfM7Osmf0NMK3ks7cAV5rZyXGnbi8zu9DMavexDHcBnzKzKXH/wteJmrJWmNlJ8fazwE6gCSjGfRgfN7O+cZPWDqC4H78HSTkFgaSWu78BXA58H9hE1LH8IXdvcfcW4G+ATwJbiPoTHij57Hzg00RNN1uBZfG6+1qGx4CvAPcT1UIOBy6NF/chCpytRM1Hm4FvxsuuAFaY2Q7gSqK+BpF3xfRgGhGRdFONQEQk5RQEIiIppyAQEUk5BYGISMplyl2AfTVo0CAfN25cuYshInJIeemllza5++DOlh1yQTBu3Djmz59f7mKIiBxSzGzlnpapaUhEJOUUBCIiKacgEBFJuUOuj6AzuVyOuro6mpqayl2URFVVVTFq1Ciy2Wy5iyIiPUiPCIK6ujpqa2sZN24c7QeL7Dncnc2bN1NXV8f48ePLXRwR6UF6RNNQU1MTAwcO7LEhAGBmDBw4sMfXekTkwOsRQQD06BBolYZjFJEDr8cEwV41N8CONaDRVkVE2klPEOR2QsN68O5/fse2bdv44Q9/uM+fu+CCC9i2bVu3l0dEZF+kJghaCtGrH8AgyOfz7/i5uXPn0q9fv24vj4jIvugRVw11RXO+SAVQLDph2L3bvu6663jrrbeYMmUK2WyWqqoq+vfvz+uvv86bb77JxRdfzKpVq2hqauLqq69m1qxZwO7hMhoaGjj//PM544wzePbZZxk5ciS//vWvqa6u7t6Cioh0oscFwY3/+xqL1+x42/xCvoWw2ALZeWD7VhGaPKIP//yho/e4/Bvf+AaLFi3i5Zdf5sknn+TCCy9k0aJFbZd53nbbbQwYMIDGxkZOOukkLrnkEgYOHNhuG0uXLuWuu+7illtuYebMmdx///1cfvnl+1ROEZF3o8cFwZ5FV9x427vkTJs2rd21/t/73vd48MEHAVi1ahVLly59WxCMHz+eKVOmAHDiiSeyYsWKhEspIhJJNAjMbDrwXSAEbnX3b3Syzkzgq0Tn6Ffc/WP7s889/eVev20jtbvqyA2YSLaq1/7sYq969dq9/SeffJLHHnuM5557jpqaGs4888xO7wWorKxsex+GIY2NjYmWUUSkVWJBYGYhMBs4F6gD5pnZHHdfXLLOBOB64HR332pmQ5IrT9wclMDlo7W1tdTX13e6bPv27fTv35+amhpef/11nn/++W7fv4jI/kiyRjANWObuywHM7G5gBrC4ZJ1PA7PdfSuAu29Irjhx01ACVw0NHDiQ008/nWOOOYbq6mqGDh3atmz69On8+Mc/ZtKkSRx55JGccsop3b5/EZH9kWQQjARWlUzXASd3WGcigJn9kaj56Kvu/tuOGzKzWcAsgDFjxry70iRYIwC48847O51fWVnJww8/3Omy1n6AQYMGsWjRorb5X/jCF7q9fCIie1Lu+wgywATgTOAy4BYz69dxJXe/2d2nuvvUwYM7fdLa3llyNQIRkUNZkkGwGhhdMj0qnleqDpjj7jl3/zPwJlEwdLu2cXo0xISISDtJBsE8YIKZjTezCuBSYE6HdX5FVBvAzAYRNRUtT6Q0cdOQagQiIu0lFgTungeuAh4BlgD3uPtrZnaTmV0Ur/YIsNnMFgNPANe6++YkyqMagYhI5xK9j8Dd5wJzO8y7oeS9A5+Pf5LVdjexgkBEpFS5O4sPGFNnsYhIp1ITBElePvpuh6EG+M53vsOuXbu6uUQiIl2XmiBIso9AQSAih7LUDDrXNsQE3d80VDoM9bnnnsuQIUO45557aG5u5q//+q+58cYb2blzJzNnzqSuro5CocBXvvIV1q9fz5o1a3j/+9/PoEGDeOKJJ7q9bCIie9PzguDh62Ddq2+bnXXHcw1UBFnIVO3bNocdC+e/bby8NqXDUD/66KPcd999vPjii7g7F110EU8//TQbN25kxIgRPPTQQ0A0BlHfvn359re/zRNPPMGgQYP2rUwiIt0kNU1D0VBDlvhFQ48++iiPPvooxx9/PCeccAKvv/46S5cu5dhjj+V3v/sdX/ziF/nDH/5A3759ky2IiEgX9bwawR7+ci8WirDuVXIV/agePDax3bs7119/PZ/5zGfetmzBggXMnTuXL3/5y5x99tnccMMNnWxBROTASk2NwAwcI4kqQekw1Oeddx633XYbDQ0NAKxevZoNGzawZs0aampquPzyy7n22mtZsGDB2z4rIlIOPa9GsAeGRUGQ8DDU559/Ph/72Mc49dRTAejduzd33HEHy5Yt49prryUIArLZLD/60Y8AmDVrFtOnT2fEiBHqLBaRsjA/xIZcmDp1qs+fP7/dvCVLljBp0qR3/Jy707JmEZ6toWrI4UkWMVFdOVYRkY7M7CV3n9rZstQ0DUHcNHSIBZ+ISNJSEwRmllgfgYjIoazHBEFXmrgcww7hGsGh1ownIoeGHhEEVVVVbN68ea8nSjcjiTuLDwR3Z/PmzVRV7ePNcCIie9EjrhoaNWoUdXV1bNy48R3Xa962jtAgs/XQ/Mu6qqqKUaNGlbsYItLD9IggyGazjB8/fq/rPXPjVYyv3M7I6+bvdV0RkbToEU1DXZWzLGExV+5iiIgcVFIVBIUgS+gt5S6GiMhBJV1BYFlCz5e7GCIiB5XUBUGmqBqBiEipdAVBWEHo6iMQESmVqiAoWpYMahoSESmVqiAoBFmyqhGIiLSTqiAoBhUEFKGgWoGISKtUBYGHFdGbgjqMRURapSoIikFrEDSXtyAiIgeRVAVBW40grxqBiEirdAVBkI3eqGlIRKRNokFgZtPN7A0zW2Zm13Wy/JNmttHMXo5//iHJ8hBWRq8KAhGRNomNPmpmITAbOBeoA+aZ2Rx3X9xh1V+6+1VJlaNUsa1pSH0EIiKtkqwRTAOWuftyd28B7gZmJLi/vbJQncUiIh0lGQQjgVUl03XxvI4uMbOFZnafmY1OsDyQaQ0C3VQmItKq3J3F/wuMc/fjgN8Bt3e2kpnNMrP5ZjZ/b08heyfe2kegpiERkTZJBsFqoPQv/FHxvDbuvtndW8/KtwIndrYhd7/Z3ae6+9TBgwe/6wJZprWzWEEgItIqySCYB0wws/FmVgFcCswpXcHMhpdMXgQsSbA8bX0EhZyuGhIRaZXYVUPunjezq4BHgBC4zd1fM7ObgPnuPgf4nJldBOSBLcAnkyoP0NZHUMg1ESa6IxGRQ0eiD69397nA3A7zbih5fz1wfZJlKBVko6ahYk5NQyIircrdWXxABW01AgWBiEirVAWBZaoAKOiqIRGRNqkKgiC+ashVIxARaZOuIMhGTUNF1QhERNqkLAjUWSwi0lGqgiCbqaDohqtGICLSJlVBkMmE5MjgejCNiEibdAVBaDSTUY1ARKREqoKgIgxoIasgEBEpkaogyARGjoyGoRYRKZGuIAgDWjyjYahFREqkKgham4b0zGIRkd1SFQSZsLVpSEEgItIqVUGQDY0WMpiCQESkTcqCIKCZrJ5QJiJSIlVBkAkDch4SFHXVkIhIq1QFQTYwWsiqaUhEpES6giC+aigoKghERFqlKgiiq4ZCTE1DIiJtUhUEbTUCNQ2JiLRJXRA0e4ZQTUMiIm1SFQRhYOQtQ+BqGhIRaZWqIADIWwWh+ghERNqkMAiyhK6mIRGRVqkLgoJlCb0AxWK5iyIiclBIXxAEFfEb1QpERCCNQWDZ+I3GGxIRgRQGQTGIg0APsBcRAVIZBGoaEhEplWgQmNl0M3vDzJaZ2XXvsN4lZuZmNjXJ8kBpEKhpSEQEEgwCMwuB2cD5wGTgMjOb3Ml6tcDVwAtJlaVUQU1DIiLtJFkjmAYsc/fl7t4C3A3M6GS9fwH+HWhKsCxtPFTTkIhIqSSDYCSwqmS6Lp7XxsxOAEa7+0PvtCEzm2Vm881s/saNG/erUB62XjWkIBARgTJ2FptZAHwbuGZv67r7ze4+1d2nDh48eL/260Fl9CavPgIREUg2CFYDo0umR8XzWtUCxwBPmtkK4BRgTtIdxmoaEhFpL8kgmAdMMLPxZlYBXArMaV3o7tvdfZC7j3P3ccDzwEXuPj/BMoGCQESkncSCwN3zwFXAI8AS4B53f83MbjKzi5La717LFappSESkVCbJjbv7XGBuh3k37GHdM5MsSxt1FouItJO6O4stE9cIFAQiIkAKg4BM3EegpiERESCFQWCtQVDQU8pERCCNQdDaWayxhkREgC4GgZldbWZ9LPJTM1tgZh9IunBJaOsjUNOQiAjQ9RrB37n7DuADQH/gCuAbiZUqQWoaEhFpr6tBYPHrBcDP3f21knmHlIpMSIuHuGoEIiJA14PgJTN7lCgIHomHjj4kn/6eCQNayCoIRERiXb2h7O+BKcByd99lZgOATyVWqgRlQiNHhmK+JX095SIinejqufBU4A1332ZmlwNfBrYnV6zkVIQBLWQo5g/I4w9ERA56XQ2CHwG7zOw9RMNGvwX8LLFSJSgTGC2exXO6s1hEBLoeBHl3d6InjP3A3WcTDSN9yMnENQJXjUBEBOh6H0G9mV1PdNnoe+OHymSTK1ZyWpuGXGMNiYgAXa8RfBRoJrqfYB3RQ2a+mVipEpQJLb5qSEEgIgJdDIL45P8LoK+ZfRBocvdDs48gDMiR0Z3FIiKxrg4xMRN4EfgIMBN4wcw+nGTBklIRGi2e0TDUIiKxrvYRfAk4yd03AJjZYOAx4L6kCpaUTBDdUKYgEBGJdLWPIGgNgdjmffjsQaX1hjI1DYmIRLpaI/itmT0C3BVPf5QOj6A8VFSEAQ1ksKIGnRMRgS4Ggbtfa2aXAKfHs2529weTK1ZyWscaMjUNiYgA+/Dwene/H7g/wbIcENnQyHlGQSAiEnvHIDCzesA7WwS4u/dJpFQJysY3lCkIREQi7xgE7n5IDiPxTlpvKLOigkBEBA7RK3/2Rza+oSxQEIiIAGkMgiCgmQxhMQfeWauXiEi6pC4IMmE0DDWg5xaLiJDCIIiahsJooqCbykREUhgEUWcxoBqBiAgJB4GZTTezN8xsmZld18nyK83sVTN72cyeMbPJSZYHdt9QBmiYCREREgwCMwuB2cD5wGTgsk5O9He6+7HuPgX4D+DbSZWnVTY06r06mmjcmvTuREQOeknWCKYBy9x9ubu3AHcTPeqyjbvvKJnsRec3r3WrbBBQ54Ojie2rkt6diMhBr8tDTLwLI4HSM20dcHLHlczss8DngQrgrM42ZGazgFkAY8aM2a9CBYGxmjgItv1lv7YlItITlL2z2N1nu/vhwBeBL+9hnZvdfaq7Tx08ePB+73N72J+cVcK2lfu9LRGRQ12SQbAaGF0yPSqetyd3AxcnWJ422TBke8VQ2KamIRGRJINgHjDBzMabWQVwKTCndAUzm1AyeSGwNMHytMmGxtaKYWoaEhEhwT4Cd8+b2VXAI0AI3Obur5nZTcB8d58DXGVm5wA5YCvwiaTKUyoTBmzJDoNtzx6I3YmIHNSS7CzG3efS4Ulm7n5Dyfurk9z/nmQDY3NmGOzaBC07oaJXOYohInJQKHtncTlkMwEbM0OjCfUTiEjKpTIIMoGxMYiDQPcSiEjKpTIIsmHA+mBINKFLSEUk5VIbBJvpC2GFrhwSkdRLZRBkQiPnBn1HKwhEJPVSGQTZICBXKEK/MQoCEUm9dAZBxsgVPA4CdRaLSLqlMggyQUC+tUawcwPkGstdJBGRskllEGRDo6W1RgCqFYhIqqU0CEpqBKB+AhFJtVQGQSYMyBdLawS6l0BE0iuVQZANjZZ8EXoPgyCru4tFJNXSGQRBQL5YhCCAfrqXQETSLZVBkAmNfCF+PLLuJRCRlEtlEGTDgJZCMZrQ3cUiknIpDYLSGsFYaFgPuabyFkpEpExSGQTRVUNxjUCXkIpIyqUyCLJhQK7guDsMnRzNXLOgvIUSESmTdAZBYADRvQRDJkNVX1ip5xeLSDqlMggyYXTYuUIRghBGnwJ/ea7MpRIRKY9UBkE2jGoEudYO47GnwqY3oWFjGUslIlIeKQ2C6LDzrZeQjjktelWtQERSKJVBkOlYIxhxPGSqFAQikkqpDIJ+1RUAbGpojmZkKmDkVHUYi0gqpTIIjhxWC8Ab6+p3zxx7KqxbCM31e/iUiEjPlMogGDewhspMwJK1O3bPHHMqeBFWvVi+gomIlEEqgyATBkwcWsvrpTWC0dPAAvUTiEjqpDIIACYNr+X1dSU1gspaGP4eWPkcFIvw4i3wveNhzctlK6OIyIGQaBCY2XQze8PMlpnZdZ0s/7yZLTazhWb2uJmNTbI8pY4a1odNDS1sqC8ZbG7MabB6Pvz3dJj7BdiyHN56/EAVSUSkLBILAjMLgdnA+cBk4DIzm9xhtT8BU939OOA+4D+SKk9HRw2POoxfX1vaYXwa5Juim8su/jH0GQUblhyoIomIlEWSNYJpwDJ3X+7uLcDdwIzSFdz9CXffFU8+D4xKsDztTBrWB6B989DE6TBjNlw1H6ZcBkOOgg2vH6giiYiURZJBMBIofRhwXTxvT/4eeLizBWY2y8zmm9n8jRu7ZxiI/r0qGNaniiWlNYIwA8dfDr0GRdODj4pqB8VCt+xTRORgdFB0FpvZ5cBU4JudLXf3m919qrtPHTx4cLft96jhte0vIe1oyGQoNMOWP3fbPkVEDjZJBsFqYHTJ9Kh4Xjtmdg7wJeAid29OsDxvM2l4H97a2EBLvtj5CkOOil43LD5whRIROcCSDIJ5wAQzG29mFcClwJzSFczseOAnRCGwIcGydOqoYbXkCs5bGxs6X2HQkdHrRvUTiEjPlVgQuHseuAp4BFgC3OPur5nZTWZ2UbzaN4HewL1m9rKZzdnD5hIxaXgnHcalKntHzzTWlUMi0oNlkty4u88F5naYd0PJ+3OS3P/eHDaoFxVhEF1CevweVhoySUEgIj3aQdFZXC6ZMGDC0N4sfqcO48FHweZlUMgduIKJiBxAqQ4CiO4wbjfmUEdDJkMxB5vfOnCFEhE5gFIfBJOG17Kxvnn3swk60pVDItLDpT4ITjlsIAB3PL+y8xUGTYxGJdWVQyLSQ6U+CI4Z2ZcLjx3OT55azrrtTW9fIVsN/cepw1hEeqzUBwHAdecfRaHo/Oejb3S+wpDJqhGISI+lIABGD6jhU2eM4/4FdSxavf3tKww+Kuoszh/QG59FRA4IBUHss+8/ggE1FfzLbxbj7u0XDpkEXoBNS8tTOBGRBCkIYn2qsvyfcyfywp+3cNsfV7RfOGRS9Kp+AhHpgRQEJS6bNobzjh7Kv/xmMb9+uWR8vEETobIP/Pmp8hVORCQhCoISYWB899LjOXn8AK655xWeejN+9kGYhSPOhqWPRs8zFhHpQRQEHVRlQ275xFQmDK3l/7njJRaviYefmDgdGtbD2pfLWj4Rke6mIOhEn6ost//dSfSqzHDNva+QKxThiHMBgzcfKXfxRES6lYJgD4bUVvGvFx/DkrU7+OETb0GvgTB6GrzZ6dM0RUQOWQqCd/CBo4cxY8oIvv/7pVET0cTpsPYV2LGme3aQb4FdW7pnWyIi75KCYC+++qGj6VeT5dr7XiF3xAeimUsf7Z6NP/ol+OGpUMi3n99cryYoETlgFAR70b9XBV+7+FheW7ODG58v4n1Hd89JurkeXr4TGtbB6vntlz37fbhzJmxatv/7ERHZCwVBF0w/Zhifed9h3PHCKv5UdTK89QTkGvdvo4vuh5b4WclLf9d+2evxQ91WPrN/+xAR6QIFQRddd/5RXDZtDN9ddTjkG+G52bDxTSgW3t0GX7odBk+CMafCspIg2LoS1r8avV/57P4XXERkLxQEXWRmfO3iYxhw9Fms8/7w+3+B2SfBv42CBT9/+wdevQ9ef6jzja1dCGsWwImfhCPOiTqgGzZEy978bfQ6/D0KAhE5IBQE+yAMjP+4dBrfmnQvFzR/ndl9r6FlwJFRp2/jtt0rbvkzPHgl3PupzgeqW3A7ZKrguJkw4dxo3rLHo9c35kZDWhx/BWxfFdUQREQSpCDYR9kw4JuXnsQ/fGQGP9w6jSvWXwZN28n98Qe7V3riXyHIRA+1+fVn2zcfteyEhffA5BlQMwCGHQe9h0bNQ43bYMUzcOQFMPa0aH3VCkQkYQqCd+lvThjFQ597L/mhx/JQYRrNz/yAb//qWRbOexpevZfCyVfC9G/AqhfgxVt2f/BPv4DmHVGzEIAZHH42vPX7eCyjfBQEgydBdX91GItI4jLlLsChbNygXtx35aksXvg1ah48j14v/ZCtL61ka9CbM38/mYljRvKjYe9j4OM3YjUDYMHPYMUf2NbvaL61oJZ5Dz4NwHePPZkjG++Ep/4dagbBqKkQBDDmNNUIRCRxqhHsJzPj6PecTHDcTGZlH+avwoWsmHwll595HGu2N/OhFR9hV87hgU+zacUibspdwanrruGBP61mcG0ljbkCM39XTZEANi+DI6dDEEYbH3sabFkOO9a+cyH+8jx873h47cHkD1hEehx729O4DnJTp071+fPn733FA23Lcvj+VOgzAq6aD9kqcoUi//vKGl54/EFqdq1m47gZnHD4MKaNH8BRw2rJhAFNuQL/9dibnPvc3zI1eJMvZq5jYe0Z1FZlGN+ylH/f/P8xe+A/cdz0v+O9RwyCnRuh1+CoSQlg+VNw16WQ2xU9M+HKZ6D/2Hd/HC27IN8U9V+ISI9hZi+5+9ROlykIutGiB6DvqGhwun209rffou/87/G1Cb9kQ1NIfVOekAK3rvsIvw3ex5d2zuTWAXdy+q7HyPcbz85JM9lVM4IhT1zL9qrR3D3sC/zdimtYVz2Bu4/+IZNG9Oe00VUMfv0O6DOS3FEXsXp7DjMYml9H1bzZMHQynPQPuwuRa4Rbz4HGrXDVPKjo1Y2/HBEpJwXBoaBYhEILZKvaz7/jEnzjG2zNV9Jv51vckT+HCbaaU8PFALxaHMfftlxH2HsQH/Sn+Grh+3yrcCkLC2P51+xPGWWbAKjzQfx3fjpH2Go+HD5N1qIrmf6t9z+xqN9fMbh3JZ/c9gOmrLsPgEUTruSZUbMIDI4Y0ptJFRsZsn0hjZM+wq5cARwG9a4kCOzA/Y5E5F17pyBItLPYzKYD3wVC4FZ3/0aH5e8DvgMcB1zq7vclWZ6DWhBAUPX2+WNPx5Y9xoCagTTMvIdgx5EsA7Y0r2bUluew4z7CUyOH06cqC34O3FfH51/7JRY6W2rG8/XaL9E/aOBDDffylfo7KARZXhv+YX7f7yPMeOsGPr/z2/xTdjiV61cxJXcfN+cvZJht4dw3b+PTrx7NWgYymG38qvIrhLaZB+5/gH/OfwInoCITMKp/NUNqK9nZXGB7Y46mXIER/ao5tm8jf1V4jiW9T2FFYQgNzTkG9KpgWJ9qhvSpZGdznvU7mthY38yg3pUcOayWI4fVEpixsaGZzQ0t1FSEjBlQw9iBNTjwl827qNu6i2wYMGV0Pwb2ruzSr7ZYdAWWyDtIrEZgZiHwJnAuUAfMAy5z98Ul64wD+gBfAOZ0JQh6bI1gT+rXwx+/C6f+v1Gz0940boVfXhENXfG+L0Cm5GS5bhHUDIQ+w3dv+5azAIeWBor9D2PFjAfIbVvLxHvPonDUB9l53ncIf/ZBqra+wZsDz2byht+wbOQMnjv6n/FV8zl+5W0c3riQ53udxR+HXEZD9UgmrH6QmdtupQ87yXnIg5np3Fv9UXo11nFG8x84M3iF1T6IFzmGJdUn8MKuYezM775uYbSt533Bq6zxgTxRnALsPolPsDoyFFjiYxk7sIbhfavYWN9Mff12qos76TVwNOMH96JfdZblG3eydEMDW3e1MHFoLe8Z1ZeJQ2tpKRRpaMrTlCvQv1cFQ2or6V9Twfr6Jv6yeRertzUyvG8VRw3rw5HDaqmuCMkXnHyxSEUYUFOZoSYb0pgrsLmhhU07m6kMA0b0q2Z4vyoqMyHFotNSiB5rWpkJMFMQSXmVpWnIzE4Fvuru58XT1wO4+791su7/AL9REJTB2oVw23lgIVz5NAw4LJr/+E3wh2/B2DOiexlm/hwmfQie+g948uvQZxTsqIPqATDujGhojGI++vzmZTDuvfiZ12MLfwl/+jm4A46HFTSPOo1swxrCzW8C4EGWXN/xbKkeS+/6t+hdv7yteDv6HskLoz6FW8iJ6+5h4KZ5ANT1n8Y9VTNZlhvI3+Tncnr9w1QXGlhUdQJ3+nk81nIM7+23mdOqVzLUtvJs8xHcv2kM65sCsuQ5JljBpMwa/pLvz5+Lw1jDQAD6hS2M6wNL6qto6jA6eKkMefKElIZUq4pMQEt+97Otw8CoqQipCKP5zfkijlNTkaF3ZYbKbAAOhfj/YlUmpCobUJkJ222+IoxqYRVhQBCAYWCQDYyKTEA2DAjMMIPAjDAwMqGRDQKCwDBa50MmDMgERiYwrOQzmcDalrVmV2BGNjTCICAs2a8BmSBo209rUc0gDOLth4aVHEQY7F5Wmo2t+w4DIyhZEG3LyARBu/XNon0HhkK2i8oVBB8Gprv7P8TTVwAnu/tVnaz7P7xDEJjZLGAWwJgxY05cuVLDLnSrta9EJ+oRU3bPa66H758YPaf5rK9EtYtWz82OBs2b+ik44W+jTuUda+GFH8HyJ2HaLJjy8d1XNm18I7qHYujR0c1y1f2i+fXrojup1y+K1tm0NKr1TDwvGoNp9Uvw9H/C5niYjr5jYNqn4zL8ICobRCE2eUY0NMeCn0H9GqLTVPt/2x5WUhhwOOGWt7BCc7tlxSBLUMztXjdTRXP/iWyqHo8XC1S2bKOyZQvZlm1km7eRLewiF9awq89h5AdMpIDh29eQ3bmWPCHbq0dT32sszZnehM3bCZu3Y16gOduX5mw/CmElmaatZFu2EhSaqc8MYEdmEDvDPgT5RoLcTqzQTH3Ql21hf3ZYHyjkyBR2ERSa2U4tm6wfW6w/hUKRbGEXmXwj272azfSl4EbRoaVQJF8oUjy0ugL3ScdWv9YwCQNrF9NtIdlhvlk03RqIbfOhLSBblxPPM4sC0Yx2n21dt3VfZhaHVfvttPscu7fXFrBx8LaGY2tAf3TqaM6YMOhd/Z4O+SAopRrBAfTnP0Qn49OvhnL91VUsRM9/MIMJH9h9j0WuCRbeHV1O+56PQd+R0fxCHt54CNa8DMOOgREnQO8hsPI5WP4ErH8tCqRRJ8HQY6B+bRQ0W1dGzWiVtdE4UFtX7A6oMBvd6NdrUFQDqhkAVf1g12bY9EY0Ci0OfUZGzW6FPGx5KxpzqpiDsCK6SzzIRE+ky8dDmFsQNdWFlbBzQ3SxQHcIMtB7WLT9lnpoboCKGrzPSKgdgYcVePMOvLkeLEOh93CKtSMoZnvhzTugqR73Avlew8jXDKNQ2Qdv2Yk17cALLeSqB5OrGUquoh/kdmHNO/DcLvKV/WmuGkpz1UDI7cSatmPN9eQq+tJcNYTmiv54MU/YtJWgcQu5TA1NVUMoBlncnXyuhWzjJgpBluZsf7AozIrukNtFsZAnn+0NRH+3FIpOvlDACi0Uw8rd890pFp1iIY9b9O/F42XFYhErtuAWULS4i7RYJCw0kik0EnoewzEKhIUcmWIzYbGZwPOExRyB5wm8gMU/bdsGig4FMuQJyZEhZ1maqaCFCnYFvdhl1TRSTdEC3B13cOLX0vdE/Vr5YtQcWShCvlCkUHSuPmcCM6aMfFf/LNQ0JFIOxQLkm6Mxp0qDNNcY3atR2Te6SACiM0Hj1igostVRLStTGYVNw3rYuTmarugdvTZuiWpUDeujE39FL8jWRNuoX7v7caoVvaNluV3RvO11UMhBVZ/ovpNCc1Sb27E6vhelNpoP0fZLakn7LchEzYftWHRfjBeiY22VqYqCNQijcjTviI+nNgr9bHU0Ym/D+miblX2iwK+sjX6HOzdBbmf0O6nqG70210PTtt2BawEE2eh3cCBlqqPyV/WBAYfDoAnQbyx4Mfp34R7/sTI1OqZuUq6rhuYBE8xsPLAauBT4WIL7Ezm4BCFU1Lx9fjY+EZQyi2oaHW/k6zuqaxcJdAf39oFVLEYn56btUNk7OtmG2egEXL8uWlZRE83PVkfT9euiWlq2JqoFVdbG4bQuehpfpio6xuoB0YOZdqyJQijIRie9XoOjE/v2VVFoFfNw2Puhdlj0+2wNs1wjDJkcDdhYUROd+OvXRdscNDHaTmVtNN24LRrssbI2apas7BOddAst0U8mDt6KmqgcFkQ/mcqovJmq6LjDiug1CKPmyCCM1mv7fRWi4Czk4203R38I5HbFIbQjes03RuVv3Br1py14PgqtzrSGYa4p+sz0r0fNsd0ssSBw97yZXQU8QnT56G3u/pqZ3QTMd/c5ZnYS8CDQH/iQmd3o7kcnVSYReQcdm/+CAHoPjn5K9R25uymunQmJFa1Ha60NBmEUOsUCrFsIdfOjVyy6vyhbA4OOTKQIuqFMRCQF3qlpSIPOiYiknIJARCTlFAQiIimnIBARSTkFgYhIyikIRERSTkEgIpJyCgIRkZQ75G4oM7ONwLsdfnQQsKkbi3OoSONxp/GYIZ3HncZjhn0/7rHuPrizBYdcEOwPM5u/pzvrerI0HncajxnSedxpPGbo3uNW05CISMopCEREUi5tQXBzuQtQJmk87jQeM6TzuNN4zNCNx52qPgIREXm7tNUIRESkAwWBiEjKpSYIzGy6mb1hZsvM7LpylycJZjbazJ4ws8Vm9pqZXR3PH2BmvzOzpfFr/3KXtbuZWWhmfzKz38TT483shfj7/qWZVZS7jN3NzPqZ2X1m9rqZLTGzU1PyXf+f+N/3IjO7y8yqetr3bWa3mdkGM1tUMq/T79Yi34uPfaGZnbCv+0tFEJhZCMwGzgcmA5eZ2eTylioReeAad58MnAJ8Nj7O64DH3X0C8Hg83dNcDSwpmf534L/c/QhgK/D3ZSlVsr4L/NbdjwLeQ3T8Pfq7NrORwOeAqe5+DNFjcC+l533f/wNM7zBvT9/t+UTPCZ0AzAJ+tK87S0UQANOAZe6+3N1bgLuBGWUuU7dz97XuviB+X090YhhJdKy3x6vdDlxclgImxMxGARcCt8bTBpwF3Bev0hOPuS/wPuCnAO7e4u7b6OHfdSwDVJtZBqgB1tLDvm93fxrY0mH2nr7bGcDPPPI80M/Mhu/L/tISBCOBVSXTdfG8HsvMxgHHAy8AQ919bbxoHTC0XOVKyHeA/x8oxtMDgW3uno+ne+L3PR7YCPx33CR2q5n1ood/1+6+GvhP4C9EAbAdeIme/33Dnr/b/T6/pSUIUsXMegP3A//o7jtKl3l0vXCPuWbYzD4IbHD3l8pdlgMsA5wA/Mjdjwd20qEZqKd91wBxu/gMoiAcAfTi7U0oPV53f7dpCYLVwOiS6VHxvB7HzLJEIfALd38gnr2+taoYv24oV/kScDpwkZmtIGryO4uo7bxf3HQAPfP7rgPq3P2FePo+omDoyd81wDnAn919o7vngAeI/g309O8b9vzd7vf5LS1BMA+YEF9ZUEHUuTSnzGXqdnHb+E+BJe7+7ZJFc4BPxO8/Afz6QJctKe5+vbuPcvdxRN/r793948ATwIfj1XrUMQO4+zpglZkdGc86G1hMD/6uY38BTjGzmvjfe+tx9+jvO7an73YO8Lfx1UOnANtLmpC6xt1T8QNcALwJvAV8qdzlSegYzyCqLi4EXo5/LiBqM38cWAo8Bgwod1kTOv4zgd/E7w8DXgSWAfcCleUuXwLHOwWYH3/fvwL6p+G7Bm4EXgcWAT8HKnva9w3cRdQHkiOq/f39nr5bwIiuinwLeJXoiqp92p+GmBARSbm0NA2JiMgeKAhERFJOQSAiknIKAhGRlFMQiIiknIJA5AAyszNbR0gVOVgoCEREUk5BINIJM7vczF40s5fN7Cfx8w4azOy/4rHwHzezwfG6U8zs+Xgs+AdLxok/wsweM7NXzGyBmR0eb753yXMEfhHfIStSNgoCkQ7MbBLwUeB0d58CFICPEw1wNt/djwaeAv45/sjPgC+6+3FEd3a2zv8FMNvd3wOcRnSnKESjwv4j0bMxDiMaK0ekbDJ7X0Ukdc4GTgTmxX+sVxMN8FUEfhmvcwfwQPxcgH7u/lQ8/3bgXjOrBUa6+4MA7t4EEG/vRXevi6dfBsYBzyR+VCJ7oCAQeTsDbnf369vNNPtKh/Xe7fgszSXvC+j/oZSZmoZE3u5x4MNmNgTanhU7luj/S+sIlx8DnnH37cBWM3tvPP8K4CmPnhBXZ2YXx9uoNLOaA3kQIl2lv0REOnD3xWb2ZeBRMwuIRoD8LNHDX6bFyzYQ9SNANCTwj+MT/XLgU/H8K4CfmNlN8TY+cgAPQ6TLNPqoSBeZWYO79y53OUS6m5qGRERSTjUCEZGUU41ARCTlFAQiIimnIBARSTkFgYhIyikIRERS7v8CszVgDV3CAU4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for Neurons_i in Neurons:\n",
    "    for Dropout_rate_i in Dropout_rate:\n",
    "        for Learning_rate_i in Learning_rate:\n",
    "            for Batch_size_i in Batch_size:\n",
    "                for Epochs_i in Epochs:\n",
    "                    for Decay_i in Decay:\n",
    "                    \n",
    "                        \n",
    "                    \n",
    "                        \n",
    "                        #### GRU_v01 #######################################\n",
    "                        print(\"------------- GRU_v01 --------------------\")\n",
    "                        X_train, X_val, y_train, y_val = prepare_training_data_quat_special(Path_dataset01_test)\n",
    "                        \n",
    "                        model = GRU_v01(X_train, y_train, Neurons_i, Dropout_rate_i,Learning_rate_i, Decay_i)\n",
    "                        \n",
    "                        \n",
    "                        individual_name = \"GRU_v01_Neurons_\" + str(Neurons_i) + \"_\" + \"_DR_\" + str(Dropout_rate_i) + \"__\" + \"LR_\" + str(Learning_rate_i) + \"__\" + \"Batch_\" + str(Batch_size_i) + \"__\" + \"Epochs_\" + str(Epochs_i) \n",
    "                        \n",
    "                        # callbacks parameter\n",
    "                        #es = EarlyStopping(monitor='val_loss', mode='min', patience=10)\n",
    "                        #mc = ModelCheckpoint(Path_results_dataset01_GRU_quat_model_id + \"/Model/\" + individual_name +  '_CUDNN_best_model.h5', monitor ='val_loss', mode='min', save_best_only=True, verbose=0)\n",
    "                        #csv_logger = CSVLogger(Path_results_dataset01_GRU_quat_model_id + \"/Model/\" + individual_name + '.csv', append=True, separator=';')\n",
    "                        \n",
    "                        # validation_data=(X_val, y_val)\n",
    "                        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), shuffle = False, batch_size=Batch_size_i, epochs=Epochs_i, verbose=1) #callbacks=[es,mc,csv_logger] , callbacks=[CustomCallback()]\n",
    "                        #val_history = model.evaluate(X_val, y_val, batch_size=1, verbose=1) #callbacks=[es,mc,csv_logger]\n",
    "\n",
    "                        save_model(model,Path_results_dataset01_GRU_quat_model_id + \"/Model/\" + individual_name )\n",
    "                        \n",
    "                        \n",
    "                        loss_history = history.history[\"loss\"]\n",
    "                        plt.plot(history.history['loss'])\n",
    "                        plt.plot(history.history['val_loss'])\n",
    "                        plt.title('model loss')\n",
    "                        plt.ylabel('loss')\n",
    "                        plt.xlabel('epoch')\n",
    "                        plt.legend(['train', 'test'], loc='upper left')\n",
    "                        plt.show()\n",
    "                        \n",
    "                        loss_history = np.array(history.history[\"loss\"]) \n",
    "                        df_loss = pd.DataFrame(loss_history)\n",
    "                        df_loss = df_loss[df_loss > 0.2]\n",
    "                        df_loss = df_loss[df_loss < 0.2]\n",
    "                        \n",
    "                        val_loss_history = np.array(history.history[\"val_loss\"]) \n",
    "                        df_val_loss = pd.DataFrame(val_loss_history)\n",
    "                        df_val_loss = df_val_loss[df_loss > 0.2]\n",
    "                        df_val_loss = df_val_loss[df_loss < 0.2]\n",
    "\n",
    "                \n",
    "                        plt.plot(df_loss)\n",
    "                        plt.plot(df_val_loss)\n",
    "                        plt.title('model loss')\n",
    "                        plt.ylabel('loss')\n",
    "                        plt.xlabel('epoch')\n",
    "                        plt.legend(['train', 'test'], loc='upper left')\n",
    "                        plt.show()\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        tf.keras.backend.clear_session()\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf7a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Neurons_i in Neurons:\n",
    "    for Dropout_rate_i in Dropout_rate:\n",
    "        for Learning_rate_i in Learning_rate:\n",
    "            for Batch_size_i in Batch_size:\n",
    "                for Epochs_i in Epochs:\n",
    "                    for Decay_i in Decay:\n",
    "                    \n",
    "                        tf.random.set_seed(0)\n",
    "                    \n",
    "                        \n",
    "                        #### Baseline #######################################\n",
    "                        print(\"------------- Baseline --------------------\")\n",
    "                        X_train, X_val, y_train, y_val = prepare_training_data_quat_special(Path_dataset01_test)\n",
    "                        \n",
    "                        #### Baseline #######################################\n",
    "                        print(\"------------- Baseline --------------------\")\n",
    "                    \n",
    "                        baseline = Baseline(X_train, y_train)\n",
    "                            \n",
    "                        baseline_history = baseline.fit(X_train, y_train, validation_data=(X_val, y_val), shuffle = False, batch_size=Batch_size_i, epochs=Epochs_i, verbose=1) #callbacks=[es,mc,csv_logger] , callbacks=[CustomCallback()]\n",
    "                    \n",
    "                        tf.keras.backend.clear_session()\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a6e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
