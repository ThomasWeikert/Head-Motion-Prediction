{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee0f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd  \n",
    "import random\n",
    "import math as mp\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Dropout\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyquaternion import Quaternion\n",
    "\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e76681a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_files(dataset_path, keyword):\n",
    "    \"\"\"\n",
    "    Generator function to recursively output the CSV files in a directory and its sub-directories.\n",
    "    Arguments:\n",
    "        dataset_path: Path to the directory containing the CSV files.\n",
    "    Outputs:\n",
    "        Paths of the found CSV files.\n",
    "    \"\"\"\n",
    "    files = []\n",
    " \n",
    "    for f in glob.glob(os.path.join(dataset_path,\"*{}*\".format(keyword))):\n",
    "        #print(f)\n",
    "        if not os.path.isdir(f):\n",
    "            file_name, extension = f.split('.')            \n",
    "            if extension == \"csv\":\n",
    "                files.append(f)\n",
    "            else:\n",
    "                logging.warn(\"Invalid file: {}. Ignoring...\".format(f))\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d81e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(path): \n",
    "    df = pd.read_csv(path, index_col=None)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a5470bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps, LAT, n_in_seq, n_out_seq):\n",
    "    X, y = list(), list()\n",
    "    \n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        stop_ix = i + n_steps\n",
    "        shift_ix = i + n_steps + LAT\n",
    "        seq_length = len(sequences) - LAT\n",
    "        if stop_ix > seq_length:\n",
    "          break\n",
    "        seq_x = sequences[i:stop_ix,:-n_in_seq]\n",
    "        seq_y = sequences[shift_ix-1, -n_out_seq:]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    return array(X), array(y)\n",
    "    #return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f18c9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_rotsonly(dataframe):\n",
    "    # multivariate data preparation\n",
    "\n",
    "    # define sequences\n",
    "    \n",
    "    \n",
    "    in_seq4 = dataframe[['qw']].to_numpy()\n",
    "    in_seq5 = dataframe[['qx']].to_numpy()\n",
    "    in_seq6 = dataframe[['qy']].to_numpy()\n",
    "    in_seq7 = dataframe[['qz']].to_numpy()\n",
    "    \n",
    "    out_seq4 = dataframe[['qw']].to_numpy()\n",
    "    out_seq5 = dataframe[['qx']].to_numpy()\n",
    "    out_seq6 = dataframe[['qy']].to_numpy()\n",
    "    out_seq7 = dataframe[['qz']].to_numpy()\n",
    "\n",
    "    # convert to [rows, columns] structure\n",
    "    in_seq4 = in_seq4.reshape((len(in_seq4), 1))\n",
    "    in_seq5 = in_seq5.reshape((len(in_seq5), 1))\n",
    "    in_seq6 = in_seq6.reshape((len(in_seq6), 1))\n",
    "    in_seq7 = in_seq7.reshape((len(in_seq7), 1))\n",
    " \n",
    "    out_seq4 = out_seq4.reshape((len(out_seq4), 1))\n",
    "    out_seq5 = out_seq5.reshape((len(out_seq5), 1))\n",
    "    out_seq6 = out_seq6.reshape((len(out_seq6), 1))\n",
    "    out_seq7 = out_seq7.reshape((len(out_seq7), 1))\n",
    "\n",
    "    # horizontally stack columns\n",
    "    dataset = hstack((in_seq4, in_seq5, in_seq6, in_seq7, out_seq4, out_seq5, out_seq6, out_seq7))\n",
    "    \n",
    "    # define number of input and output sequences\n",
    "    n_in_seq = 4\n",
    "    n_out_seq = 4\n",
    "        \n",
    "    # choose a number of time steps\n",
    "    n_steps = 5 \n",
    "    \n",
    "    LAT_in_rows = 6 \n",
    "\n",
    "    # convert into input/output\n",
    "    X, y = split_sequences(dataset, n_steps, LAT_in_rows, n_in_seq, n_out_seq)\n",
    "\n",
    "    print(\"X shape is: \" + str(X.shape))\n",
    "    print(\"y shape is: \" + str(y.shape))\n",
    "    print(\"------\")\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca77790",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"C:/Users/weikert1/Documents/Thesis/Project_V06_26072021/Results/01_Dataset/\"\n",
    "\n",
    "for csv in get_csv_files(path_train, \"3\"):\n",
    "        csv_df = load_df(csv)\n",
    "        in_seq1 = csv_df[['qw_pred']].to_numpy()\n",
    "        in_seq2 = csv_df[['qx_pred']].to_numpy()\n",
    "        in_seq3 = csv_df[['qy_pred']].to_numpy()\n",
    "        in_seq4 = csv_df[['qz_pred']].to_numpy()\n",
    "\n",
    "#for csv in get_csv_files(path_train, \"y_true\"):\n",
    "        csv_df = load_df(csv)\n",
    "        in_seq5 = csv_df[['qw_true']].to_numpy()\n",
    "        in_seq6 = csv_df[['qx_true']].to_numpy()\n",
    "        in_seq7 = csv_df[['qy_true']].to_numpy()\n",
    "        in_seq8 = csv_df[['qz_true']].to_numpy()\n",
    "\n",
    "        \n",
    "        dataset_in = hstack((in_seq1, in_seq2, in_seq3, in_seq4))\n",
    "        dataset_in_q1 = ([Quaternion(q) for q in dataset_in])\n",
    "        dataset_in_q2 = ([Quaternion(q) for q in dataset_in[1:]])\n",
    "        delta_in = np.array([(q1/q2).elements for q1, q2 in zip(dataset_in_q1,dataset_in_q2)])\n",
    "        \n",
    "        \n",
    "        dataset_true = hstack((in_seq5, in_seq6, in_seq7, in_seq8))\n",
    "        dataset_out_q1 = ([Quaternion(q) for q in dataset_out])\n",
    "        dataset_out_q2 = ([Quaternion(q) for q in dataset_out[1:]])\n",
    "        delta_out = np.array([(q1/q2).elements for q1, q2 in zip(dataset_out_q1,dataset_out_q2)])\n",
    "        \n",
    "        dataset_pred = array(dataset_pred)\n",
    "        dataset_true = array(dataset_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce2dea07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4b88b512bfdf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_pred' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e942b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
