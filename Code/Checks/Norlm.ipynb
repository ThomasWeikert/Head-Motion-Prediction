{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d963f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd  \n",
    "import random\n",
    "import math as mp\n",
    "import pickle\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from pickle import load\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from pickle import dump\n",
    "\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU\n",
    "from tensorflow.keras.layers import Dense,Flatten,Dropout, MaxPooling1D\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyquaternion import Quaternion\n",
    "\n",
    "import toml\n",
    "\n",
    "from math import floor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ad066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f770fbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Init Configuration\n",
    "######################################################################\n",
    "\n",
    "config_path = 'C:/Users/weikert1/Documents/Thesis/Project_V06_26072021/Code/V06_config.toml'\n",
    "cfg = toml.load(config_path)\n",
    "\n",
    "# General \n",
    "n_in_seq = cfg['n_in_seq_euler']\n",
    "n_out_seq = cfg['n_out_seq_euler']\n",
    "n_steps = cfg['n_steps'] \n",
    "LAT_in_rows = cfg['LAT_in_rows'] #6*16,66ms= 64ms\n",
    "\n",
    "# model ID --> specify this for every new model\n",
    "model_id = cfg['4_2_model_ID']\n",
    "model_type = cfg['4_model_type']\n",
    "\n",
    "# Directories \n",
    "Path_dataset01_test = cfg['Path_dataset01']\n",
    "Path_results_dataset01_GRU_euler_model_id = cfg['Path_results_dataset01'] + model_type + \"/\" + model_id  + \"/\" \n",
    "\n",
    "# Tensorflow configuration\n",
    "#config = tf.compat.v1.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#sess = tf.compat.v1.Session(config=config)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b04f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(path): \n",
    "    df = pd.read_csv(path, index_col=None)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_csv_files(dataset_path, keyword):\n",
    "    \"\"\"\n",
    "    Generator function to recursively output the CSV files in a directory and its sub-directories.\n",
    "    Arguments:\n",
    "        dataset_path: Path to the directory containing the CSV files.\n",
    "    Outputs:\n",
    "        Paths of the found CSV files.\n",
    "    \"\"\"\n",
    "    files = []\n",
    " \n",
    "    for f in glob.glob(os.path.join(dataset_path,\"*{}*\".format(keyword))):\n",
    "        #print(f)\n",
    "        if not os.path.isdir(f):\n",
    "            file_name, extension = f.split('.')            \n",
    "            if extension == \"csv\":\n",
    "                files.append(f)\n",
    "            else:\n",
    "                logging.warn(\"Invalid file: {}. Ignoring...\".format(f))\n",
    "\n",
    "    return files\n",
    "\n",
    "def split_sequences(sequences, n_steps, LAT, n_in_seq, n_out_seq):\n",
    "    X, y = list(), list()\n",
    "    \n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        stop_ix = i + n_steps\n",
    "        shift_ix = i + n_steps + LAT\n",
    "        seq_length = len(sequences) - LAT\n",
    "        if stop_ix > seq_length:\n",
    "          break\n",
    "        seq_x = sequences[i:stop_ix,:-n_in_seq]\n",
    "        seq_y = sequences[shift_ix-1, -n_out_seq:]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    return array(X), array(y)\n",
    "    #return X, y\n",
    "\n",
    "\n",
    "def get_sequence_quat(dataframe, dataset_path_train):\n",
    "    # multivariate data preparation\n",
    "\n",
    "    # define sequences   \n",
    "    in_seq4 = dataframe[['qw']].to_numpy()\n",
    "    in_seq5 = dataframe[['qx']].to_numpy()\n",
    "    in_seq6 = dataframe[['qy']].to_numpy()\n",
    "    in_seq7 = dataframe[['qz']].to_numpy()\n",
    "    \n",
    "    out_seq4 = dataframe[['qw']].to_numpy()\n",
    "    out_seq5 = dataframe[['qx']].to_numpy()\n",
    "    out_seq6 = dataframe[['qy']].to_numpy()\n",
    "    out_seq7 = dataframe[['qz']].to_numpy()\n",
    "    \n",
    "    # convert to [rows, columns] structure   \n",
    "    in_seq4 = in_seq4.reshape((len(in_seq4), 1))\n",
    "    in_seq5 = in_seq5.reshape((len(in_seq5), 1))\n",
    "    in_seq6 = in_seq6.reshape((len(in_seq6), 1))\n",
    "    in_seq7 = in_seq7.reshape((len(in_seq7), 1))\n",
    "    \n",
    "    out_seq4 = out_seq4.reshape((len(out_seq4), 1))\n",
    "    out_seq5 = out_seq5.reshape((len(out_seq5), 1))\n",
    "    out_seq6 = out_seq6.reshape((len(out_seq6), 1))\n",
    "    out_seq7 = out_seq7.reshape((len(out_seq7), 1))\n",
    "        \n",
    "\n",
    "    # horizontally stack columns\n",
    "    #dataset = hstack((in_seq1, in_seq2, in_seq3, in_seq4, in_seq5, in_seq6, in_seq7, out_seq1, out_seq2, out_seq3, out_seq4, out_seq5, out_seq6, out_seq7))\n",
    "    dataset = hstack((in_seq4, in_seq5, in_seq6, in_seq7,out_seq4, out_seq5, out_seq6, out_seq7))\n",
    "\n",
    "    #scaler_obj = scaler(dataset)\n",
    "    #normalized_dataset = scaler_obj.transform(dataset)\n",
    "    #print(normalized_dataset)\n",
    "    \n",
    "    # define number of input and output sequences\n",
    "    n_in_seq = 4\n",
    "    n_out_seq = 4\n",
    "        \n",
    "    # choose a number of time steps\n",
    "    n_steps = 15 \n",
    "    \n",
    "    dt = 5 # sampling rate in ms\n",
    "    LAT_in_ms = 100\n",
    "    LAT_in_rows = 5 #6*16,66ms= 64ms\n",
    "\n",
    "    # convert into input/output\n",
    "    X, y = split_sequences(dataset, n_steps, LAT_in_rows, n_in_seq, n_out_seq)\n",
    "\n",
    "    print(\"X shape is: \" + str(X.shape))\n",
    "    print(\"y shape is: \" + str(y.shape))\n",
    "    print(\"------\")\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "119f216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data_quats(dataset_path_train):\n",
    " \n",
    "    X_train_result = [] \n",
    "    y_train_result = [] \n",
    "    \n",
    "    X_val_result = [] \n",
    "    y_val_result = [] \n",
    "\n",
    "    \n",
    "    for csv in get_csv_files(dataset_path_train, \"*train\"):\n",
    "        csv_df = load_df(csv)\n",
    "        X_single, y_single = get_sequence_quat(csv_df, dataset_path_train)\n",
    "        \n",
    "        # split into X_single_train, X_single_test and y_single_train, y_single_test with 80/20 split\n",
    "        train_size = int(len(X_single) * 0.80)\n",
    "        test_size = len(X_single) - train_size\n",
    "        X_train, X_val = X_single[0:train_size,:], X_single[train_size:len(X_single),:] \n",
    "        y_train, y_val =  y_single[0:train_size,:], y_single[train_size:len(y_single),:]     \n",
    "        \n",
    "        X_train_result.append(X_train)\n",
    "        y_train_result.append(y_train)\n",
    "        \n",
    "        X_val_result.append(X_val)\n",
    "        y_val_result.append(y_val)\n",
    "        \n",
    "    X_train_result = np.vstack((X_train_result))\n",
    "    y_train_result = np.vstack((y_train_result))\n",
    "\n",
    "    X_val_result = np.vstack((X_val_result))\n",
    "    y_val_result = np.vstack((y_val_result))\n",
    "        \n",
    "    return X_train_result, X_val_result, y_train_result, y_val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5995ee8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is: (71405, 15, 4)\n",
      "y shape is: (71405, 4)\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Load Training Data\n",
    "######################################################################\n",
    "\n",
    "#pd.set_option(\"display.precision\", 2)\n",
    "X_train, X_val, y_train, y_val = prepare_training_data_quats(Path_dataset01_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b57a866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde9a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149eed41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66653cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f82fdfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc0a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b19cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd98208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c19bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
