{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47d0afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd  \n",
    "import random\n",
    "import math as mp\n",
    "import pickle\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from pickle import load\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from pickle import dump\n",
    "\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU\n",
    "from tensorflow.keras.layers import Dense,Flatten,Dropout, MaxPooling1D\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyquaternion import Quaternion\n",
    "\n",
    "import toml\n",
    "\n",
    "from math import floor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "fca33d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = 0\n",
    "        value_max = 2*np.pi\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    " \n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "    return dataset\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "12e00aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(path): \n",
    "    df = pd.read_csv(path, index_col=None)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "6a406e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_files(dataset_path, keyword):\n",
    "    \"\"\"\n",
    "    Generator function to recursively output the CSV files in a directory and its sub-directories.\n",
    "    Arguments:\n",
    "        dataset_path: Path to the directory containing the CSV files.\n",
    "    Outputs:\n",
    "        Paths of the found CSV files.\n",
    "    \"\"\"\n",
    "    files = []\n",
    " \n",
    "    for f in glob.glob(os.path.join(dataset_path,\"*{}*\".format(keyword))):\n",
    "        #print(f)\n",
    "        if not os.path.isdir(f):\n",
    "            file_name, extension = f.split('.')            \n",
    "            if extension == \"csv\":\n",
    "                files.append(f)\n",
    "            else:\n",
    "                logging.warn(\"Invalid file: {}. Ignoring...\".format(f))\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "81d98ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps, LAT, n_in_seq, n_out_seq):\n",
    "    X, y = list(), list()\n",
    "    \n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        stop_ix = i + n_steps\n",
    "        shift_ix = i + n_steps + LAT\n",
    "        seq_length = len(sequences) - LAT\n",
    "        if stop_ix > seq_length:\n",
    "          break\n",
    "        seq_x = sequences[i:stop_ix,:-n_in_seq]\n",
    "        seq_y = sequences[shift_ix-1, -n_out_seq:]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    return array(X), array(y)\n",
    "    #return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "d8c252ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Preprocessing steps: \n",
    "# 1.) Euler(deg) \n",
    "# 2.) Euler(deg) + |Euler(deg).min|%360 \n",
    "# 3.) Euler(deg:0-360) into Euler(rad:0-2pi) \n",
    "# 4.) Scale between: 0-1 \n",
    "##############################################\n",
    "\n",
    "def get_sequence_euler_radiant(dataframe, dataset_path_train):\n",
    "    # multivariate data preparation\n",
    "\n",
    "    # define sequences\n",
    "    in_seq4 = dataframe[['pitch']].to_numpy()\n",
    "    in_seq5 = dataframe[['roll']].to_numpy()\n",
    "    in_seq6 = dataframe[['yaw']].to_numpy()\n",
    "    \n",
    "    in_seq4_minimum = np.min(in_seq4)\n",
    "    in_seq5_minimum = np.min(in_seq5)\n",
    "    in_seq6_minimum = np.min(in_seq6)\n",
    "\n",
    "    out_seq4 = dataframe[['pitch']].to_numpy()\n",
    "    out_seq5 = dataframe[['roll']].to_numpy()\n",
    "    out_seq6 = dataframe[['yaw']].to_numpy()\n",
    "\n",
    "    print(\"Original y_true : \" + str(in_seq4[19]))\n",
    "    \n",
    "    out_seq4_minimum = np.min(out_seq4)\n",
    "    out_seq5_minimum = np.min(out_seq5)\n",
    "    out_seq6_minimum = np.min(out_seq6)\n",
    "    \n",
    "    in_seq4 = (in_seq4 + np.abs(in_seq4_minimum))%360\n",
    "    in_seq5 = (in_seq5 + np.abs(in_seq5_minimum))%360\n",
    "    in_seq6 = (in_seq6 + np.abs(in_seq6_minimum))%360\n",
    "    \n",
    "    out_seq4 = (out_seq4 + np.abs(out_seq4_minimum))%360\n",
    "    out_seq5 = (out_seq5 + np.abs(out_seq5_minimum))%360\n",
    "    out_seq6 = (out_seq6 + np.abs(out_seq6_minimum))%360\n",
    "    \n",
    "     # transform into radiant\n",
    "    in_seq4 = np.deg2rad(in_seq4)\n",
    "    in_seq5 = np.deg2rad(in_seq5)\n",
    "    in_seq6 = np.deg2rad(in_seq6)\n",
    "    \n",
    "    out_seq4 = np.deg2rad(out_seq4)\n",
    "    out_seq5 = np.deg2rad(out_seq5)\n",
    "    out_seq6 = np.deg2rad(out_seq6)    \n",
    "    \n",
    "    print(\"Shifted rad y_true : \" + str(in_seq4[19]))\n",
    "    \n",
    "    # convert to [rows, columns] structure   \n",
    "    in_seq4 = in_seq4.reshape((len(in_seq4), 1))\n",
    "    in_seq5 = in_seq5.reshape((len(in_seq5), 1))\n",
    "    in_seq6 = in_seq6.reshape((len(in_seq6), 1))\n",
    "    \n",
    "    out_seq4 = out_seq4.reshape((len(out_seq4), 1))\n",
    "    out_seq5 = out_seq5.reshape((len(out_seq5), 1))\n",
    "    out_seq6 = out_seq6.reshape((len(out_seq6), 1))\n",
    "\n",
    "    # horizontally stack columns\n",
    "    #dataset = hstack((in_seq1, in_seq2, in_seq3, in_seq4, in_seq5, in_seq6, in_seq7, out_seq1, out_seq2, out_seq3, out_seq4, out_seq5, out_seq6, out_seq7))\n",
    "    dataset = hstack((in_seq4, in_seq5, in_seq6,out_seq4, out_seq5, out_seq6))\n",
    "\n",
    "    minmax = dataset_minmax(dataset)\n",
    "    normalized_dataset = normalize_dataset(dataset, minmax)\n",
    "    \n",
    "    # define number of input and output sequences\n",
    "    n_in_seq = 3\n",
    "    n_out_seq = 3\n",
    "        \n",
    "    # choose a number of time steps\n",
    "    n_steps = 15 \n",
    "    \n",
    "    LAT_in_rows = 4 #5*16,66ms= 83ms\n",
    "\n",
    "    # convert into input/output\n",
    "    X, y = split_sequences(normalized_dataset, n_steps, LAT_in_rows, n_in_seq, n_out_seq)\n",
    "\n",
    "    print(\"X shape is: \" + str(X.shape))\n",
    "    print(\"y shape is: \" + str(y.shape))\n",
    "    print(\"------\")\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "7570e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data_euler(dataset_path_train):\n",
    " \n",
    "    X_train_result = [] \n",
    "    y_train_result = [] \n",
    "    \n",
    "    X_val_result = [] \n",
    "    y_val_result = [] \n",
    "\n",
    "    \n",
    "    for csv in get_csv_files(dataset_path_train, \"*train\"):\n",
    "        csv_df = load_df(csv)\n",
    "        X_single, y_single = get_sequence_euler_radiant(csv_df, dataset_path_train)\n",
    "        \n",
    "        # split into X_single_train, X_single_test and y_single_train, y_single_test with 80/20 split\n",
    "        train_size = int(len(X_single) * 0.80)\n",
    "        test_size = len(X_single) - train_size\n",
    "        X_train, X_val = X_single[0:train_size,:], X_single[train_size:len(X_single),:] \n",
    "        y_train, y_val =  y_single[0:train_size,:], y_single[train_size:len(y_single),:]     \n",
    "        \n",
    "        X_train_result.append(X_train)\n",
    "        y_train_result.append(y_train)\n",
    "        \n",
    "        X_val_result.append(X_val)\n",
    "        y_val_result.append(y_val)\n",
    "        \n",
    "    X_train_result = np.vstack((X_train_result))\n",
    "    y_train_result = np.vstack((y_train_result))\n",
    "\n",
    "    X_val_result = np.vstack((X_val_result))\n",
    "    y_val_result = np.vstack((y_val_result))\n",
    "        \n",
    "    return X_train_result, X_val_result, y_train_result, y_val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "8d0bfea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Init Configuration\n",
    "######################################################################\n",
    "\n",
    "config_path = 'C:/Users/weikert1/Documents/Thesis/Project_V06_26072021/Code/V06_config.toml'\n",
    "cfg = toml.load(config_path)\n",
    "\n",
    "# General \n",
    "n_in_seq = cfg['n_in_seq_euler']\n",
    "n_out_seq = cfg['n_out_seq_euler']\n",
    "n_steps = cfg['n_steps'] \n",
    "LAT_in_rows = cfg['LAT_in_rows'] #6*16,66ms= 64ms\n",
    "\n",
    "# model ID --> specify this for every new model\n",
    "model_id = cfg['4_2_model_ID']\n",
    "model_type = cfg['4_model_type']\n",
    "\n",
    "# Directories \n",
    "Path_dataset01_test = cfg['Path_dataset01']\n",
    "Path_results_dataset01_GRU_euler_model_id = cfg['Path_results_dataset01'] + model_type + \"/\" + model_id  + \"/\" \n",
    "\n",
    "# Tensorflow configuration\n",
    "#config = tf.compat.v1.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#sess = tf.compat.v1.Session(config=config)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "a21358b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y_true : [5.775161]\n",
      "Shifted rad y_true : [0.10087727]\n",
      "X shape is: (71406, 15, 3)\n",
      "y shape is: (71406, 3)\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Load Training Data\n",
    "######################################################################\n",
    "\n",
    "#pd.set_option(\"display.precision\", 2)\n",
    "X_train, X_val, y_train, y_val = prepare_training_data_euler(Path_dataset01_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "0673f7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01605512, 0.99298423, 0.2367753 ])"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "1da09d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scaler_objects(path_scaler_objects):\n",
    "\n",
    "    out_seq4_scaler = load(open(path_scaler_objects + 'out_seq4_scaler.pkl', 'rb'))\n",
    "    out_seq5_scaler = load(open(path_scaler_objects + 'out_seq5_scaler.pkl', 'rb'))\n",
    "    out_seq6_scaler = load(open(path_scaler_objects + 'out_seq6_scaler.pkl', 'rb'))\n",
    "    \n",
    "    out_seq4_scaler_min = out_seq4_scaler.data_min_\n",
    "    out_seq5_scaler_min = out_seq5_scaler.data_min_\n",
    "    out_seq6_scaler_min = out_seq6_scaler.data_min_\n",
    "    \n",
    "    out_seq4_scaler_max = out_seq4_scaler.data_max_\n",
    "    out_seq5_scaler_max = out_seq5_scaler.data_max_\n",
    "    out_seq6_scaler_max = out_seq6_scaler.data_max_\n",
    "    \n",
    "    return out_seq4_scaler_min, out_seq5_scaler_min, out_seq6_scaler_min, out_seq4_scaler_max, out_seq5_scaler_max, out_seq6_scaler_max\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "14d2c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_inverse_transform(y_true, y_pred):\n",
    "    path_scaler_objects = Path_dataset01_test + \"Scaler_objects_euler/\" \n",
    "    w_min, x_min, y_min, w_max, x_max, y_max = load_scaler_objects(path_scaler_objects)\n",
    "    w1, x1, y1 = tf.unstack(y_true, axis=-1)\n",
    "    w2, x2, y2 = tf.unstack(y_pred, axis=-1)\n",
    "    print(\"shifted scaled value of w1 : \" + str(w1))\n",
    "    w1, w2 = (w1 - K.constant(w_min, dtype=tf.float64)) / K.constant(w_max - w_min,dtype=tf.float64) , (w2 - K.constant(w_min, dtype=tf.float64)) / K.constant(w_max - w_min , dtype=tf.float64)\n",
    "    x1, x2 = (x1 - K.constant(x_min, dtype=tf.float64)) / K.constant(x_max - x_min, dtype=tf.float64) , (x2 - K.constant(x_min, dtype=tf.float64)) / K.constant(x_max - x_min, dtype=tf.float64)\n",
    "    y1, y2 = (y1 - K.constant(y_min, dtype=tf.float64)) / K.constant(y_max - y_min, dtype=tf.float64) , (y2 - K.constant(y_min, dtype=tf.float64)) / K.constant(y_max - y_min, dtype=tf.float64)\n",
    "    print(\"re-inversed value of w1 : \" + str(w1))\n",
    "    \n",
    "    print(w1)\n",
    "    y_true = tf.stack((w1, x1, y1), axis=-1)\n",
    "    print(y_true)\n",
    "    y_pred = tf.stack((w2, x2, y2), axis=-1)\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "96204d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "d16cc080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01605512 0.99298423 0.2367753 ]\n",
      "tf.Tensor([0.01605512 0.99298423 0.2367753 ], shape=(3,), dtype=float64)\n",
      "tf.Tensor([0.02362982 0.99185506 0.23680525], shape=(3,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "y_true = y_train[1]\n",
    "y_test = y_train[1000]\n",
    "\n",
    "print(y_true)\n",
    "\n",
    "y_true_tf = tf.convert_to_tensor(y_true, dtype= np.double)\n",
    "print(y_true_tf)\n",
    "y_test_tf = tf.convert_to_tensor(y_test, dtype= np.double)\n",
    "print(y_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "9e050824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_loss(y_true,y_pred):\n",
    "    rotation_loss = tf.sqrt(tf.nn.l2_loss(y_true-y_pred))\n",
    "    return rotation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "9c60d429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.005415354114272378>"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation_loss(y_true_tf,y_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "58d3944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_np = y_true_tf.numpy()\n",
    "y2_np = y_test_tf.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "e476c983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.57470833e-03,  1.12916667e-03, -2.99444444e-05])"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y1_np - y2_np\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "58ea93a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.57470833e-03  1.12916667e-03 -2.99444444e-05]\n",
      "-0.007574708333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0076584672334569365"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "y = y1_np - y2_np\n",
    "print(y)\n",
    "\n",
    "a = y[0]\n",
    "print(a)\n",
    "b = y[1]\n",
    "c = y[2]\n",
    "norm = math.sqrt(a*a + b*b + c*c)\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "b97d0ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18327736320134166"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy.linalg import norm\n",
    "\n",
    "d = array([a,b,c])\n",
    "norm(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "0e7fac09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-362-ddeae6f486b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ml2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "a = array([1, 2, 3])\n",
    "print(a)\n",
    "l2 = norm(a)\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "1e2bfcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r1\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "r2\n",
      "[[ 0. -1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "Angle dif\n",
      "[[ 0.00000000e+00  1.57079633e+00  0.00000000e+00]\n",
      " [-1.57079633e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  2.22044605e-16]]\n",
      "norm\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-482-7faa6746bd71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"norm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mangle_dif_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"this = expected*sqrt(2)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.linalg import logm, expm\n",
    "\n",
    "\n",
    "\n",
    "r1 = R.from_euler('xyz', [0,0,0],degrees = True).as_matrix()\n",
    "r2 = R.from_euler('xyz', [0,0,90],degrees = True).as_matrix()\n",
    "\n",
    "print(\"r1\")\n",
    "print(r1)\n",
    "print(\"r2\")\n",
    "print(r2)\n",
    "\n",
    "angle_dif_mat = logm(np.matmul(r1,r2.transpose()))\n",
    "print(\"Angle dif\")\n",
    "print(angle_dif_mat)\n",
    "\n",
    "print(\"norm\")\n",
    "a = norm(angle_dif_mat)\n",
    "\n",
    "print(\"this = expected*sqrt(2)\")\n",
    "print(\"expected = pi/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eff9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d4e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83fcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf572b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1eb728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
