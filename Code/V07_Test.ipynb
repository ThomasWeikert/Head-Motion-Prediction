{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd  \n",
    "import random\n",
    "import math as mp\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from pickle import load\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyquaternion import Quaternion\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import model_from_yaml\n",
    "from tensorflow.keras.layers import Dense,Flatten,Dropout\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from math import floor\n",
    "import toml\n",
    "\n",
    "\n",
    "from ipynb.fs.full.V07_Utils  import *\n",
    "from ipynb.fs.full.V07_Prepare_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Load Configuration\n",
    "######################################################################\n",
    "\n",
    "config_path = 'C:/Users/Thomas Weikert/Documents/Thesis/Project_V07_24092021/Code/V07_config.toml'\n",
    "cfg = toml.load(config_path)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Load Parameters \n",
    "######################################################################\n",
    "\n",
    "# General\n",
    "n_steps = cfg['n_steps']\n",
    "LAT_in_rows = cfg['LAT_in_rows']\n",
    "\n",
    "# Quats\n",
    "n_in_seq_quat = cfg['n_in_seq_quat']\n",
    "n_out_seq_quat = cfg['n_out_seq_quat']\n",
    "n_features_quat = cfg['n_features_quat']\n",
    "\n",
    "# Position\n",
    "n_in_seq_pos = cfg['n_in_seq_pos']\n",
    "n_out_seq_pos = cfg['n_out_seq_pos']\n",
    "n_features_pos = cfg['n_features_pos']\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Load Directories \n",
    "######################################################################\n",
    "\n",
    "# Dataset path \n",
    "Path_dataset01_test = cfg['Path_dataset01']\n",
    "\n",
    "# GRU euler\n",
    "Path_results_dataset01_GRU_euler = cfg['Path_results_dataset01'] + \"GRU_euler/\"\n",
    "\n",
    "# GRU quat\n",
    "Path_results_dataset01_GRU_quat = cfg['Path_results_dataset01'] + \"GRU_quat/\"\n",
    "\n",
    "# GRU pos\n",
    "Path_results_dataset01_GRU_pos = cfg['Path_results_dataset01'] + \"GRU_pos/\"\n",
    "\n",
    "# Baseline quat\n",
    "Path_results_dataset01_Baseline_quat = cfg['Path_results_dataset01'] + \"Baseline_quat/\"\n",
    "\n",
    "# Baseline pos\n",
    "Path_results_dataset01_Baseline_pos = cfg['Path_results_dataset01'] + \"Baseline_pos/\"\n",
    "\n",
    "# Tensorflow configuration\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## \n",
    "# Baseline Position \n",
    "##########################\n",
    "\n",
    "def baseline_pos(path_test, result_path, n_features, n_steps, LAT_in_rows):\n",
    "\n",
    "    for csv in get_csv_files(path_test, \"full_cont\"):\n",
    "            print(csv)\n",
    "            basename = os.path.splitext(os.path.basename(csv))[0]\n",
    "            \n",
    "            preds_pos = []\n",
    "            true_pos = []\n",
    "            \n",
    "            X_test, y_test = prepare_testing_data_pos_special(csv)     \n",
    "            for i in range(len(X_test)):\n",
    "                x_input = X_test[i]\n",
    "                \n",
    "                # Make single prediction and load single groundtruth \n",
    "                phat = x_input[-1]\n",
    "                ptrue = y_test[i]\n",
    "\n",
    "                # Store single prediction and corresponding groundtruth in list\n",
    "                preds_pos.append(rhat)\n",
    "                true_pos.append(rtrue)\n",
    "\n",
    "            # Create an array \n",
    "            preds_pos = array(preds_pos)\n",
    "            true_pos = array(true_pos)\n",
    "            true_pos = true_pos.reshape((true_pos.shape[0],1, true_pos.shape[1]))\n",
    "\n",
    "            # Create dataframe for prediction & gt per timestamp and concate results \n",
    "            df_preds_pos = pd.DataFrame(np.row_stack(preds_pos), columns= cfg['pos_coords_pred'])\n",
    "            df_true_pos = pd.DataFrame(np.row_stack(true_pos), columns= cfg['pos_coords_true'])\n",
    "            df_pos_result = pd.concat([df_preds_pos, df_true_pos], axis=1)\n",
    "\n",
    "            # Save result to csv\n",
    "            df_rot_result.to_csv(os.path.join(result_path, basename + '_result_pos.csv'), index=False)\n",
    "            print(\"Single Predictions are saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## \n",
    "# Baseline Position Delta\n",
    "##########################\n",
    "\n",
    "def baseline_delta_pos(path_test, result_path, n_features, n_steps, LAT_in_rows):\n",
    "    \n",
    "    \n",
    "     for csv in get_csv_files(path_test, \"full_cont\"):\n",
    "        print(csv)\n",
    "        basename = os.path.splitext(os.path.basename(csv))[0]\n",
    "    \n",
    "        preds_pos = []\n",
    "        true_pos = []\n",
    "        \n",
    "        X_train, y_train = prepare_testing_data_pos_special_delta(csv)   \n",
    "        for i in range(len(X_train)):\n",
    "            x_input = X_train[i]\n",
    "\n",
    "            # Make single prediction and load single groundtruth \n",
    "            phat = x_input[-1:]\n",
    "            ptrue = (0,0,0)\n",
    "\n",
    "            # Store single prediction and corresponding groundtruth in list\n",
    "            preds_pos.append(phat)\n",
    "            true_pos.append(ptrue)\n",
    "            \n",
    "        # Create an array \n",
    "        preds_pos = array(preds_pos)\n",
    "        true_pos = array(true_pos)\n",
    "        true_pos = true_pos.reshape((true_pos.shape[0],1, true_pos.shape[1]))\n",
    "\n",
    "        # Create dataframe for prediction per timestamp  \n",
    "        df_preds_pos = pd.DataFrame(np.row_stack(preds_pos), columns= cfg['pos_coords_pred'])\n",
    "        df_true_pos = pd.DataFrame(np.row_stack(true_pos), columns= cfg['pos_coords_true'])\n",
    "        df_pos_result = pd.concat([df_preds_pos, df_true_pos], axis=1)\n",
    "\n",
    "        # Save result to csv\n",
    "        df_pos_result.to_csv(os.path.join(result_path, basename + '_result_pos.csv'), index=False)\n",
    "        print(\"Single Predictions are saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## \n",
    "# Baseline Quaternion \n",
    "##########################\n",
    "\n",
    "def baseline_quat(path_test, result_path, n_features, n_steps, LAT_in_rows):\n",
    "\n",
    "    for csv in get_csv_files(path_test, \"full*contin\"):\n",
    "        print(csv)\n",
    "        basename = os.path.splitext(os.path.basename(csv))[0]\n",
    "    \n",
    "        preds_rot = []\n",
    "        true_rot = []\n",
    "        \n",
    "        X_test, y_test = prepare_testing_data_quat_special(csv)   \n",
    "        for i in range(len(X_test)):\n",
    "            x_input = X_test[i]\n",
    "                \n",
    "            # Make single prediction and load single groundtruth \n",
    "            rhat = x_input[-1]\n",
    "            rtrue = y_train[i]\n",
    "\n",
    "            # Store single prediction and corresponding groundtruth in list\n",
    "            preds_rot.append(yhat)\n",
    "            true_rot.append(ytrue)\n",
    "                \n",
    "        # Create an array \n",
    "        preds_rot = array(preds_rot)\n",
    "        true_rot = array(true_rot)\n",
    "        true_rot = true_rot.reshape((true_rot.shape[0],1, true_rot.shape[1]))\n",
    "        \n",
    "        # Create dataframe for prediction & gt per timestamp and concate results \n",
    "        df_preds_rot = pd.DataFrame(np.row_stack(preds_rot), columns= cfg['quat_coords'])\n",
    "        df_true_rot = pd.DataFrame(np.row_stack(true_rot), columns= cfg['quat_coords'])\n",
    "        df_rot_result = pd.concat([df_preds_rot, df_true_rot], axis=1)\n",
    "\n",
    "        # Save result to csv\n",
    "        df_rot_result.to_csv(os.path.join(result_path, basename + '_result_rot_baseline.csv'), index=False)\n",
    "        print(\"Single Predictions are saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## \n",
    "# Baseline Quaternion Delta\n",
    "##########################\n",
    "\n",
    "def baseline_quat_delta(path_test, result_path, n_features, n_steps, LAT_in_rows):\n",
    "    \n",
    "    \n",
    "     for csv in get_csv_files(path_test, \"full*contin\"):\n",
    "        print(csv)\n",
    "        basename = os.path.splitext(os.path.basename(csv))[0]\n",
    "        \n",
    "        preds_rot = []\n",
    "        true_rot = []\n",
    "\n",
    "        X_test, y_test = prepare_testing_data_quat_special_delta(csv)   \n",
    "        \n",
    "        for i in range(len(X_test)):\n",
    "            x_input = X_test[i]\n",
    "\n",
    "            # Make single prediction and load single groundtruth \n",
    "            rhat = x_input[-1:]\n",
    "            rtrue = (1,0,0,0)\n",
    "\n",
    "            # Store single prediction and corresponding groundtruth in list\n",
    "            preds_rot.append(rhat)\n",
    "            true_rot.append(rtrue)\n",
    "            \n",
    "         # Create an array \n",
    "        preds_rot = array(preds_rot)\n",
    "        true_rot = array(true_rot)\n",
    "        true_rot = true_rot.reshape((true_rot.shape[0],1, true_rot.shape[1]))\n",
    "\n",
    "        # Create dataframe for prediction per timestamp (preds_rot) and add euler angles \n",
    "        df_preds_rot = pd.DataFrame(np.row_stack(preds_rot), columns= cfg['quat_coords_pred'])\n",
    "        df_true_rot = pd.DataFrame(np.row_stack(true_rot), columns= cfg['quat_coords_true'])\n",
    "        df_rot_result = pd.concat([df_preds_rot, df_true_rot], axis=1)\n",
    "\n",
    "        # Save result to csv\n",
    "        df_rot_result.to_csv(os.path.join(result_path, basename + '_result_delta_rot.csv'), index=False)\n",
    "        print(\"Single Predictions are saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## \n",
    "# Neural Net Quaternion \n",
    "##########################\n",
    "\n",
    "def make_prediction_quat(path_test, result_path, n_features, n_steps, LAT_in_rows):\n",
    "\n",
    "    for csv in get_csv_files(path_test, \"full*contin\"):\n",
    "            print(csv)\n",
    "            basename = os.path.splitext(os.path.basename(csv))[0]\n",
    "\n",
    "            preds_rot = []\n",
    "            true_rot = []\n",
    "\n",
    "            X_test, y_test = prepare_testing_data_quat_special(csv)   \n",
    "            for i in range(len(X_test)):\n",
    "\n",
    "                x_input = X_test[i]\n",
    "\n",
    "                # Reshape x_input to fit tensor\n",
    "                x_input = x_input.reshape((1, n_steps, n_features))\n",
    "\n",
    "                # Make single prediction and load single groundtruth \n",
    "                rhat = loaded_model.predict(x_input, verbose=0)\n",
    "                rtrue = y_test[i]\n",
    "\n",
    "                # Store single prediction and corresponding groundtruth in list\n",
    "                preds_rot.append(rhat)\n",
    "                true_rot.append(rtrue)\n",
    "\n",
    "            # Create an array \n",
    "            preds_rot = array(preds_rot)\n",
    "            true_rot = array(true_rot)\n",
    "            true_rot = true_rot.reshape((true_rot.shape[0],1, true_rot.shape[1]))\n",
    "\n",
    "            # Create dataframe for prediction per timestamp (preds_rot) \n",
    "            df_preds_rot = pd.DataFrame(np.row_stack(preds_rot), columns= cfg['quat_coords_pred'])\n",
    "            df_true_rot = pd.DataFrame(np.row_stack(true_rot), columns= cfg['quat_coords_true'])\n",
    "            df_rot_result = pd.concat([df_preds_rot, df_true_rot], axis=1)\n",
    "\n",
    "            # Save result to csv\n",
    "            df_rot_result.to_csv(os.path.join(result_path, basename + '_result_rot.csv'), index=False)\n",
    "            print(\"Single Predictions are saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## \n",
    "# Neural Net Quaternion Delta\n",
    "##########################\n",
    "      \n",
    "def make_prediction_delta(path_test, result_path, n_features, n_steps, LAT_in_rows):\n",
    "\n",
    "    for csv in get_csv_files(path_test, \"full*contin\"):\n",
    "            print(csv)\n",
    "            basename = os.path.splitext(os.path.basename(csv))[0]\n",
    "\n",
    "            preds_rot = []\n",
    "            true_rot = []\n",
    "\n",
    "            X_test, y_test = prepare_testing_data_quat_special_delta(csv)   \n",
    "            for i in range(len(X_test)):\n",
    "\n",
    "                x_input = X_test[i]\n",
    "\n",
    "                # Reshape x_input to fit tensor\n",
    "                x_input = x_input.reshape((1, n_steps, n_features))\n",
    "\n",
    "                # Make single prediction and load single groundtruth \n",
    "                rhat = loaded_model.predict(x_input, verbose=0)\n",
    "                rtrue = y_test[i]\n",
    "\n",
    "                # Store single prediction and corresponding groundtruth in list\n",
    "                preds_rot.append(rhat)\n",
    "                true_rot.append(rtrue)\n",
    "\n",
    "            # Create an array \n",
    "            preds_rot = array(preds_rot)\n",
    "            true_rot = array(true_rot)\n",
    "            true_rot = true_rot.reshape((true_rot.shape[0],1, true_rot.shape[1]))\n",
    "\n",
    "            # Create dataframe for prediction per timestamp (preds_rot) and add euler angles \n",
    "            df_preds_rot = pd.DataFrame(np.row_stack(preds_rot), columns= cfg['quat_coords_pred'])\n",
    "            df_true_rot = pd.DataFrame(np.row_stack(true_rot), columns= cfg['quat_coords_true'])\n",
    "            df_rot_result = pd.concat([df_preds_rot, df_true_rot], axis=1)\n",
    "\n",
    "            # Save result to csv\n",
    "            df_rot_result.to_csv(os.path.join(result_path, basename + '_result_rot.csv'), index=False)\n",
    "            print(\"Single Predictions are saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## \n",
    "# Neural Net Position  \n",
    "##########################\n",
    "        \n",
    "def make_prediction_pos(path_test, result_path, n_features, n_steps, LAT_in_rows):\n",
    "     \n",
    "    for csv in get_csv_files(path_test, \"*full*contin*\"):\n",
    "        print(csv)\n",
    "        basename = os.path.splitext(os.path.basename(csv))[0]\n",
    "    \n",
    "        preds_pos = []\n",
    "        true_pos = []\n",
    "        \n",
    "        X_test, y_test = prepare_testing_data_pos_special(csv)   \n",
    "        for i in range(len(X_test)):\n",
    "            x_input = X_test[i]\n",
    "\n",
    "            # Reshape x_input to fit tensor\n",
    "            x_input = x_input.reshape((1, n_steps, n_features))\n",
    "\n",
    "            # Make single prediction and load single groundtruth \n",
    "            rhat = loaded_model.predict(x_input, verbose=0)\n",
    "            rtrue = y_test[i]\n",
    "\n",
    "            # Store single prediction and corresponding groundtruth in list\n",
    "            preds_pos.append(rhat)\n",
    "            true_pos.append(rtrue)\n",
    "\n",
    "\n",
    "        # Create an array \n",
    "        preds_pos = array(preds_pos)\n",
    "        true_pos = array(true_pos)\n",
    "        true_pos = true_pos.reshape((true_pos.shape[0],1, true_pos.shape[1]))\n",
    "\n",
    "        # Create dataframe for prediction per timestamp (preds_rot) and add euler angles \n",
    "        df_preds_pos = pd.DataFrame(np.row_stack(preds_pos), columns= cfg['pos_coords_pred'])\n",
    "        df_true_pos = pd.DataFrame(np.row_stack(true_pos), columns= cfg['pos_coords_true'])\n",
    "        df_pos_result = pd.concat([df_preds_pos, df_true_pos], axis=1)\n",
    "\n",
    "        # Save result to csv\n",
    "        df_pos_result.to_csv(os.path.join(result_path, basename + '_result_pos.csv'), index=False)\n",
    "        print(\"Single Predictions are saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## \n",
    "# Neural Net Position Delta \n",
    "##########################\n",
    "\n",
    "def make_prediction_delta_pos(path_test, result_path, n_features, n_steps, LAT_in_rows):\n",
    "     \n",
    "    for csv in get_csv_files(path_test, \"*full*contin\"):\n",
    "        print(csv)\n",
    "        basename = os.path.splitext(os.path.basename(csv))[0]\n",
    "    \n",
    "        preds_pos = []\n",
    "        true_pos = []\n",
    "        \n",
    "        X_test, y_test = prepare_testing_data_pos_special_delta(csv)   \n",
    "        for i in range(len(X_test)):\n",
    "            x_input = X_test[i]\n",
    "\n",
    "            # Reshape x_input to fit tensor\n",
    "            x_input = x_input.reshape((1, n_steps, n_features))\n",
    "\n",
    "            # Make single prediction and load single groundtruth \n",
    "            rhat = loaded_model.predict(x_input, verbose=0)\n",
    "            rtrue = y_test[i]\n",
    "\n",
    "            # Store single prediction and corresponding groundtruth in list\n",
    "            preds_pos.append(rhat)\n",
    "            true_pos.append(rtrue)\n",
    "\n",
    "\n",
    "        # Create an array \n",
    "        preds_pos = array(preds_pos)\n",
    "        true_pos = array(true_pos)\n",
    "        true_pos = true_pos.reshape((true_pos.shape[0],1, true_pos.shape[1]))\n",
    "\n",
    "        # Create dataframe for prediction per timestamp (preds_rot) and add euler angles \n",
    "        df_preds_pos = pd.DataFrame(np.row_stack(preds_pos), columns= cfg['pos_coords_pred'])\n",
    "        df_true_pos = pd.DataFrame(np.row_stack(true_pos), columns= cfg['pos_coords_true'])\n",
    "        df_pos_result = pd.concat([df_preds_pos, df_true_pos], axis=1)\n",
    "\n",
    "        # Save result to csv\n",
    "        df_pos_result.to_csv(os.path.join(result_path, basename + '_result_delta_pos.csv'), index=False)\n",
    "        print(\"Single Predictions are saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU quat ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Provide Model ID\n",
    "model_spec = \"Delta_GRU_v01_Neurons_32__DR_0.5__LR_0.01__Batch_128__Epochs_20\"\n",
    "model_ID = cfg['6_1_model_ID']\n",
    "###################################################\n",
    "\n",
    "# Result Path \n",
    "Path_results_dataset01_GRU_quat_model_id_traces =  Path_results_dataset01_GRU_quat + \"/\" + model_ID + \"/Traces/\" \n",
    "\n",
    "# Load Model Weights\n",
    "Path_results_dataset01_GRU_quat_model_id_model_weights = Path_results_dataset01_GRU_quat + model_ID + \"/Model/\" + model_spec + \"_CUDNN_best_model\" \n",
    "\n",
    "# Load Model Architecture \n",
    "Path_results_dataset01_GRU_quat_model_id_model_arch = Path_results_dataset01_GRU_quat + model_ID + \"/Model/\" + model_spec + \"_CUDNN_arch\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load YAML and create model\n",
    "yaml_file = open(Path_results_dataset01_GRU_quat_model_id_model_arch + '.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(Path_results_dataset01_GRU_quat_model_id_model_weights + '.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Prediction Dataset 01\n",
    "# Delta\n",
    "###################################################\n",
    "\n",
    "make_prediction_delta(Path_dataset01_test, \n",
    "                         Path_results_dataset01_GRU_quat_model_id_traces,\n",
    "                         n_features_quat,\n",
    "                         n_steps,\n",
    "                         LAT_in_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Prediction Dataset 01\n",
    "# Absolut\n",
    "###################################################\n",
    "\n",
    "make_prediction_delta(Path_dataset01_test, \n",
    "                         Path_results_dataset01_GRU_quat_model_id_traces,\n",
    "                         n_features_quat,\n",
    "                         n_steps,\n",
    "                         LAT_in_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU pos ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Provide Model ID\n",
    "model_spec = \"Delta_GRU_v01_Neurons_32__DR_0.1__LR_0.01__Batch_128__Epochs_100\"\n",
    "model_ID = cfg['7_1_model_ID']\n",
    "###################################################\n",
    "\n",
    "# Result Path \n",
    "Path_results_dataset01_GRU_pos_model_id_traces =  Path_results_dataset01_GRU_pos + \"/\" + model_ID + \"/Traces/\" \n",
    "\n",
    "# Load Model Weights\n",
    "Path_results_dataset01_GRU_pos_model_id_model_weights = Path_results_dataset01_GRU_pos + model_ID + \"/Model/\" + model_spec + \"_CUDNN_best_model\" \n",
    "\n",
    "# Load Model Architecture \n",
    "Path_results_dataset01_GRU_pos_model_id_model_arch = Path_results_dataset01_GRU_pos + model_ID + \"/Model/\" + model_spec + \"_CUDNN_arch\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load YAML and create model\n",
    "yaml_file = open(Path_results_dataset01_GRU_pos_model_id_model_arch + '.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(Path_results_dataset01_GRU_pos_model_id_model_weights + '.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Prediction Dataset 01\n",
    "# Delta\n",
    "###################################################\n",
    "make_prediction_delta_pos(Path_dataset01_test, \n",
    "                         Path_results_dataset01_GRU_pos_model_id_traces,\n",
    "                         n_features_pos,\n",
    "                         n_steps,\n",
    "                         LAT_in_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Prediction Dataset 01\n",
    "# Absolut\n",
    "###################################################\n",
    "make_prediction_pos(Path_dataset01_test, \n",
    "                         Path_results_dataset01_GRU_pos_model_id_traces,\n",
    "                         n_features_pos,\n",
    "                         n_steps,\n",
    "                         LAT_in_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Quat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Traces\n",
    "Path_results_dataset01_Baseline_quat_model_id_traces =  Path_results_dataset01_Baseline_quat + \"/Traces/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Dataset 01\n",
    "baseline_quat_delta(Path_dataset01_test,\n",
    "              Path_results_dataset01_Baseline_quat_model_id_traces,\n",
    "              n_features_quat,\n",
    "              n_steps,\n",
    "              LAT_in_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Traces\n",
    "Path_results_dataset01_Baseline_pos_model_id_traces =  Path_results_dataset01_Baseline_pos + \"/Traces/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Thomas Weikert/Documents/Thesis/Project_V07_24092021/Data/01_Dataset\\1_full_continious.csv\n",
      "34990\n",
      "34989\n",
      "X shape is: (34960, 30, 3)\n",
      "y shape is: (34960, 3)\n",
      "------\n",
      "Single Predictions are saved\n",
      "C:/Users/Thomas Weikert/Documents/Thesis/Project_V07_24092021/Data/01_Dataset\\2_full_continious.csv\n",
      "14990\n",
      "14989\n",
      "X shape is: (14960, 30, 3)\n",
      "y shape is: (14960, 3)\n",
      "------\n",
      "Single Predictions are saved\n",
      "C:/Users/Thomas Weikert/Documents/Thesis/Project_V07_24092021/Data/01_Dataset\\3_full_continious.csv\n",
      "44990\n",
      "44989\n",
      "X shape is: (44960, 30, 3)\n",
      "y shape is: (44960, 3)\n",
      "------\n",
      "Single Predictions are saved\n",
      "C:/Users/Thomas Weikert/Documents/Thesis/Project_V07_24092021/Data/01_Dataset\\4_full_continious.csv\n",
      "69990\n",
      "69989\n",
      "X shape is: (69960, 30, 3)\n",
      "y shape is: (69960, 3)\n",
      "------\n",
      "Single Predictions are saved\n",
      "C:/Users/Thomas Weikert/Documents/Thesis/Project_V07_24092021/Data/01_Dataset\\5_full_continious.csv\n",
      "19990\n",
      "19989\n",
      "X shape is: (19960, 30, 3)\n",
      "y shape is: (19960, 3)\n",
      "------\n",
      "Single Predictions are saved\n"
     ]
    }
   ],
   "source": [
    "# Prediction Dataset 01\n",
    "baseline_delta_pos(Path_dataset01_test,\n",
    "               Path_results_dataset01_Baseline_pos_model_id_traces,\n",
    "               n_features_pos,\n",
    "               n_steps,\n",
    "               LAT_in_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Thomas Weikert/Documents/Thesis/Project_V07_24092021/Data/01_Dataset\\1_full_continious.csv\n",
      "34995\n",
      "34994\n",
      "X shape is: (34965, 30, 3)\n",
      "y shape is: (34965, 3)\n",
      "------\n",
      "(34965, 3)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-30d731aae3b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                \u001b[0mn_features_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                \u001b[0mn_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                LAT_in_rows)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-275202a28453>\u001b[0m in \u001b[0;36mbaseline_pos\u001b[1;34m(path_test, result_path, n_features, n_steps, LAT_in_rows)\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[1;31m# Make single prediction and load single groundtruth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mphat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                 \u001b[0mptrue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[1;31m# Store single prediction and corresponding groundtruth in list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Prediction Dataset 01\n",
    "baseline_pos(Path_dataset01_test,\n",
    "               Path_results_dataset01_Baseline_pos_model_id_traces,\n",
    "               n_features_pos,\n",
    "               n_steps,\n",
    "               LAT_in_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
